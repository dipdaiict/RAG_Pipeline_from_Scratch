{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerieval Augmentation Generations (RAG) Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Essential Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import fitz\n",
    "import random\n",
    "import textwrap\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm \n",
    "from spacy.lang.en import English\n",
    "from time import perf_counter as time\n",
    "from timeit import default_timer as timer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File human-nutrition-text.pdf already exists in our System.\n"
     ]
    }
   ],
   "source": [
    "# Get PDF document\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist on our PC, Hold on We are Downloading...\")\n",
    "\n",
    "  # The URL of the PDF you want to download\n",
    "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "  # The local filename to save the downloaded file\n",
    "  filename = pdf_path\n",
    "\n",
    "  # Send a GET request to the URL\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Check if the request was successful\n",
    "  if response.status_code == 200:\n",
    "      with open(filename, \"wb\") as file:\n",
    "          file.write(response.content)\n",
    "      print(f\"The file has been downloaded and saved as: {filename}\")\n",
    "  else:\n",
    "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} already exists in our System.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Documents and Applying Text Formatting:\n",
    "\n",
    "-- fitz library utilizedn for reading pdf file. -> (!pip install PyMuPDF fitz pypdf)\n",
    "\n",
    "-- Source code PyMuODF: https://github.com/pymupdf/pymupdf\n",
    "\n",
    "-- pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022a056dee544b27a2f20556e36a8ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 29,\n",
       "  'page_word_count': 4,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\n",
    "    \n",
    "    Text formatting might be different for each documents.\"\"\"\n",
    "    \n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "    \n",
    "    # Write down other formatting here if required.....\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),    # No. of Words in the Page\n",
    "                                \"page_word_count\": len(text.split(\" \")),  # No. of Words in Page\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")), # No. Sentences in the Page\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]   # Extracting the First Two Dict from List."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Random Samples from List of Dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Sample:\n",
      " {'page_number': 979, 'page_char_count': 1715, 'page_word_count': 298, 'page_sentence_count_raw': 11, 'page_token_count': 428.75, 'text': '• Amino Acid Supplements. Certain amino acid supplements,  which are often taken by bodybuilders among others, can  increase the risk of consuming too much protein. An  occasional amino acid drink in the place of a meal is not a  problem. However, problems may arise if you add the  supplement to your existing diet. Most Americans receive two  to three times the amount of protein required on a daily basis  from their existing diets—taking amino acid supplements just  adds to the excess. Also, certain amino acids share the same  transport systems in the absorption process; therefore, a  concentrated excess of one amino acid obtained from a  supplement may increase the probability of decreased  absorption of another amino acid that uses the same transport  system. This could lead to deficiency in the competing amino  acid.  Supplement Claims and Restrictions  The Food and Drug Administration (FDA) regulates supplements,  but it treats them like food rather than pharmaceuticals. Dietary  supplements must meet the FDA’s Good Manufacturing Standards,  but are not required to meet the standards for drugs, although  some companies do so voluntarily. Also, although supplement  manufacturers are allowed to say a particular ingredient may reduce  the risk of a disease or disorder, or that it might specifically target  certain body systems, these claims are not approved by the FDA.  This is why labels that make structural and functional claims are  required to carry a disclaimer saying the product is not intended  “to diagnose, treat, cure, or prevent any disease.” In addition, in the  United States, supplements are taken off the market only after the  Food Supplements and Food Replacements  |  979'}\n",
      "--------------------------------------------------------\n",
      "Sample: 2\n",
      "Sample:\n",
      " {'page_number': 85, 'page_char_count': 1765, 'page_word_count': 309, 'page_sentence_count_raw': 12, 'page_token_count': 441.25, 'text': 'Blood’s Function in the Body and in Metabolism  Support  You know you cannot live without blood, and that your heart pumps  your blood over a vast network of veins and arteries within your  body, carrying oxygen to your cells. However, beyond these basic  facts, what do you know about your blood?  Blood transports absorbed nutrients to cells and waste products  from cells. It supports cellular metabolism by transporting  synthesized macromolecules from one cell type to another and  carrying waste products away from cells. Additionally, it transports  molecules, such as hormones, allowing for communication between  organs. The volume of blood coursing throughout an adult human  body is about 5 liters (1.3 US gallons) and accounts for approximately  8 percent of human body weight.  What Makes Up Blood and How Do These  Substances Support Blood Function?  Blood is about 78 percent water and 22 percent solids by volume.  The liquid part of blood is called plasma and it is mostly water (95  percent), but also contains proteins, ions, glucose, lipids, vitamins,  minerals, waste products, gases, enzymes, and hormones. We have  learned that the protein albumin is found in high concentrations  in the blood. Albumin helps maintain fluid balance between blood  and tissues, as well as helping to maintain a constant blood pH. We  have also learned that the water component of blood is essential for  its actions as a transport vehicle, and that the electrolytes carried  in blood help to maintain fluid balance and a constant pH.  Furthermore, the high water content of blood helps maintain body  temperature, and the constant flow of blood distributes heat  throughout the body. Blood is exceptionally good at temperature  The Cardiovascular System  |  85'}\n",
      "--------------------------------------------------------\n",
      "Sample: 3\n",
      "Sample:\n",
      " {'page_number': -10, 'page_char_count': 521, 'page_word_count': 99, 'page_sentence_count_raw': 4, 'page_token_count': 130.25, 'text': 'Cheryl Gibby  Cheryl Gibby was born and raised in Hawai‘i and is a wife and  mother of three. She received her BA, MS in Nutritional Sciences,  and PhD in Nutrition from the University of Hawai‘i at Mānoa. She  has served as an instructor for the introductory Nutrition course  at the University of Hawai‘i at Mānoa, and her research interests  include infant and child health, dental and bone health, mobile  health  interventions,  school  nutrition  policies,  and  online  education.  xxxii  |  About the Contributors'}\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = random.sample(pages_and_texts, k=3)\n",
    "for idx, sample in enumerate(x):\n",
    "    print(f\"Sample: {idx+1}\")\n",
    "    print(f\"Sample:\\n {sample}\")\n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0          -41               29                4                        1   \n",
       "1          -40                0                1                        1   \n",
       "2          -39              320               54                        1   \n",
       "3          -38              212               32                        1   \n",
       "4          -37              797              145                        2   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              7.25                      Human Nutrition: 2020 Edition  \n",
       "1              0.00                                                     \n",
       "2             80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...  \n",
       "3             53.00  Human Nutrition: 2020 Edition by University of...  \n",
       "4            199.25  Contents  Preface  University of Hawai‘i at Mā...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Stat:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.59</td>\n",
       "      <td>198.89</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.44</td>\n",
       "      <td>95.75</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.75</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1232.50</td>\n",
       "      <td>215.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>308.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1605.25</td>\n",
       "      <td>271.25</td>\n",
       "      <td>14.00</td>\n",
       "      <td>401.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.59           198.89                     9.97   \n",
       "std         348.86           560.44            95.75                     6.19   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.75           134.00                     4.00   \n",
       "50%         562.50          1232.50           215.00                    10.00   \n",
       "75%         864.25          1605.25           271.25                    14.00   \n",
       "max        1166.00          2308.00           429.00                    32.00   \n",
       "\n",
       "       page_token_count  \n",
       "count           1208.00  \n",
       "mean             287.15  \n",
       "std              140.11  \n",
       "min                0.00  \n",
       "25%              190.69  \n",
       "50%              308.12  \n",
       "75%              401.31  \n",
       "max              577.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Description Stat:\")\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing: Splitting Pages into Sentences: (Group of 10 Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence., I like elephants.]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentencizer:\n",
    "nlp = English()  # English Instance: Already Imported\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer \n",
    "nlp.add_pipe(\"sentencizer\")   # Turning Text Text into Sentences.\n",
    "\n",
    "# Create document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence. I like elephants.\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "# Print out our sentences split\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 559,\n",
       " 'page_char_count': 864,\n",
       " 'page_word_count': 137,\n",
       " 'page_sentence_count_raw': 8,\n",
       " 'page_token_count': 216.0,\n",
       " 'text': 'Image by  Allison  Calabrese /  CC BY 4.0  Korsakoff syndrome can cause similar symptoms as beriberi such  as confusion, loss of coordination, vision changes, hallucinations,  and may progress to coma and death. This condition is specific  to alcoholics as diets high in alcohol can cause thiamin deficiency.  Other individuals at risk include individuals who also consume diets  typically low in micronutrients such as those with eating disorders,  elderly, and individuals who have gone through gastric bypass  surgery.5  Figure 9.10 The Role of Thiamin  Figure 9.11 Beriberi, Thiamin Deficiency  5.\\xa0Fact Sheets for Health Professionals: Thiamin. National  Institute of Health, Office of Dietary Supplements.  \\xa0https:/ /ods.od.nih.gov/factsheets/Thiamin- HealthProfessional/. Updated Feburary 11, 2016.  Accessed October 22, 2017.  Water-Soluble Vitamins  |  559'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Text:\n",
    "pages_and_texts[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd286191339347bdadaee0e8ad532150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing: Splitting Pages into Sentences: (Group of 10 Sentences)\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all sentences are strings (the default type is a spaCy datatype)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # Count the sentences\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1113,\n",
       "  'page_char_count': 1621,\n",
       "  'page_word_count': 256,\n",
       "  'page_sentence_count_raw': 17,\n",
       "  'page_token_count': 405.25,\n",
       "  'text': 'not others. The Diabetes Prevention Trial that studied lifestyle and  drug interventions in more than three thousand participants who  were at high risk for Type 2 diabetes found that intensive lifestyle  intervention reduced the chances of getting Type 2 diabetes by 58  percent.11  Gestational Diabetes  During pregnancy some women develop gestational diabetes.  Gestational diabetes is characterized by high blood-glucose levels  and insulin resistance. The exact cause is not known but does  involve the effects of pregnancy hormones on how cells respond  to insulin. Gestational diabetes can cause pregnancy complications  and it is common practice for healthcare practitioners to screen  pregnant women for this metabolic disorder. The disorder normally  ceases when the pregnancy is over, but the National Diabetes  Information Clearing House notes that women who had gestational  diabetes have between a 40 and 60 percent likelihood of developing  Type 2 diabetes within the next ten years.12 Gestational diabetes not  only affects the health of a pregnant woman but also is associated  with an increased risk of obesity and Type 2 diabetes in her child.  11.\\xa0Knowler WC. (2002). Reduction in the Incidence of Type  2 Diabetes with Lifestyle Intervention or Metformin. The  New England Journal of Medicine, 346(6), 393–403.  http:/ /www.nejm.org/doi/full/10.1056/NEJMoa012512.  Accessed April 15, 2018.  12.\\xa0Diabetes Overview. National Institute of Diabetes and  Digestive and Kidney Disease.  https:/ /www.niddk.nih.gov/health-information/ diabetes/overview. Accessed April 15, 2018. \\xa0 Threats to Health  |  1113',\n",
       "  'sentences': ['not others.',\n",
       "   'The Diabetes Prevention Trial that studied lifestyle and  drug interventions in more than three thousand participants who  were at high risk for Type 2 diabetes found that intensive lifestyle  intervention reduced the chances of getting Type 2 diabetes by 58  percent.11  Gestational Diabetes  During pregnancy some women develop gestational diabetes.',\n",
       "   ' Gestational diabetes is characterized by high blood-glucose levels  and insulin resistance.',\n",
       "   'The exact cause is not known but does  involve the effects of pregnancy hormones on how cells respond  to insulin.',\n",
       "   'Gestational diabetes can cause pregnancy complications  and it is common practice for healthcare practitioners to screen  pregnant women for this metabolic disorder.',\n",
       "   'The disorder normally  ceases when the pregnancy is over, but the National Diabetes  Information Clearing House notes that women who had gestational  diabetes have between a 40 and 60 percent likelihood of developing  Type 2 diabetes within the next ten years.12 Gestational diabetes not  only affects the health of a pregnant woman but also is associated  with an increased risk of obesity and Type 2 diabetes in her child.',\n",
       "   ' 11.',\n",
       "   '\\xa0Knowler WC. (',\n",
       "   '2002).',\n",
       "   'Reduction in the Incidence of Type  2 Diabetes with Lifestyle Intervention or Metformin.',\n",
       "   'The  New England Journal of Medicine, 346(6), 393–403.',\n",
       "   ' http:/ /www.nejm.org/doi/full/10.1056/NEJMoa012512.',\n",
       "   ' Accessed April 15, 2018.',\n",
       "   ' 12.',\n",
       "   '\\xa0Diabetes Overview.',\n",
       "   'National Institute of Diabetes and  Digestive and Kidney Disease.',\n",
       "   ' https:/ /www.niddk.nih.gov/health-information/ diabetes/overview.',\n",
       "   'Accessed April 15, 2018.',\n",
       "   '\\xa0 Threats to Health  |  1113'],\n",
       "  'page_sentence_count_spacy': 19}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Sample View:\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.59</td>\n",
       "      <td>198.89</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.15</td>\n",
       "      <td>10.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.44</td>\n",
       "      <td>95.75</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.11</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.75</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.69</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1232.50</td>\n",
       "      <td>215.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>308.12</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1605.25</td>\n",
       "      <td>271.25</td>\n",
       "      <td>14.00</td>\n",
       "      <td>401.31</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.59           198.89                     9.97   \n",
       "std         348.86           560.44            95.75                     6.19   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.75           134.00                     4.00   \n",
       "50%         562.50          1232.50           215.00                    10.00   \n",
       "75%         864.25          1605.25           271.25                    14.00   \n",
       "max        1166.00          2308.00           429.00                    32.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count           1208.00                    1208.00  \n",
       "mean             287.15                      10.32  \n",
       "std              140.11                       6.30  \n",
       "min                0.00                       0.00  \n",
       "25%              190.69                       5.00  \n",
       "50%              308.12                      10.00  \n",
       "75%              401.31                      15.00  \n",
       "max              577.00                      28.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics:\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chunking: Chunking is the method of breaking down the large files into more manageable segments/chunks so the LLM applications can get proper context and the retrieval can be easy.\n",
    "\n",
    "-- Different Types of Chunking:\n",
    "\n",
    "⮕ Level 1 : Fixed Size Chunking\n",
    "This is the most crude and simplest method of segmenting the text. It breaks down the text into chunks of a specified number of characters, regardless of their content or structure. Langchain and llamaindex framework offer CharacterTextSplitter and SentenceSplitter (default to spliting on sentences) classes for this chunking technique.\n",
    "\n",
    "⮕ Level 2: Recursive Chunking\n",
    "While Fixed size chunking is easier to implement, it doesn’t consider the structure of text. Recursive chunking offers an alternative.\n",
    "In this method, we divide the text into smaller chunk in a hierarchical and iterative manner using a set of separators. Langchain framework offers RecursiveCharacterTextSplitter class, which splits text using default separators (“\\n\\n”, “\\n”, “ “,””)\n",
    "\n",
    "⮕ Level 3 : Document Based Chunking\n",
    "In this chunking method, we split a document based on its inherent structure. This approach considers the flow and structure of content but may not be as effective documents lacking clear structure.\n",
    "\n",
    "⮕ Level 4: Semantic Chunking\n",
    "All above three levels deals with content and structure of documents and necessitate maintaining constant value of chunk size. This chunking method aims to extract semantic meaning from embeddings and then assess the semantic relationship between these chunks. The core idea is to keep together chunks that are semantic similar.\n",
    "Llamindex has SemanticSplitterNodeParse class that allows to split the document into chunks using contextual relationship between chunks.\n",
    "\n",
    "⮕ Level 5: Agentic Chunking\n",
    "This chunking strategy explore the possibility to use LLM to determine how much and what text should be included in a chunk based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunking our Senteces into 10 Group:\n",
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "# Create a function to split lists of texts recursively into chunk size\n",
    "# e.g. [20] -> [10, 10] or [25] -> [10, 10, 5]\n",
    "def split_list(input_list: list[str],\n",
    "               slice_size: int=num_sentence_chunk_size) -> list[list[str]]:\n",
    "    return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Testing the above function:\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eddf386c88a49b487f52bedf4e2d9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)  \n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])   # No. of chunks Created after Splitting the sentences into 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 632,\n",
       "  'page_char_count': 639,\n",
       "  'page_word_count': 120,\n",
       "  'page_sentence_count_raw': 6,\n",
       "  'page_token_count': 159.75,\n",
       "  'text': 'Age Group  RDA (mg/day) UL (mg/day)  Infants (0–6 months)  100*  –  Infants (6–12 months)  275*  –  Children (1–3 years)  460  3,000  Children (4–8 years)  500  3,000  Children (9–13 years)  1,250  4,000  Adolescents (14–18 years)  1,250  4,000  Adults (19–70 years)  700  4,000  Adults (> 70 years)  700  3,000  * denotes Adequate Intake  Micronutrient Information Center: Phosphorus. Oregon State  University, Linus Pauling Institute. http:/ /lpi.oregonstate.edu/mic/ minerals/phosphorus. Updated in July 2013. Accessed October 22,  2017.  Dietary Sources of Phosphorus  Table 10.4 Phosphorus Content of Various Foods  632  |  Phosphorus',\n",
       "  'sentences': ['Age Group  RDA (mg/day) UL (mg/day)  Infants (0–6 months)  100*  –  Infants (6–12 months)  275*  –  Children (1–3 years)  460  3,000  Children (4–8 years)  500  3,000  Children (9–13 years)  1,250  4,000  Adolescents (14–18 years)  1,250  4,000  Adults (19–70 years)  700  4,000  Adults (> 70 years)  700  3,000  * denotes Adequate Intake  Micronutrient Information Center: Phosphorus.',\n",
       "   'Oregon State  University, Linus Pauling Institute.',\n",
       "   'http:/ /lpi.oregonstate.edu/mic/ minerals/phosphorus.',\n",
       "   'Updated in July 2013.',\n",
       "   'Accessed October 22,  2017.',\n",
       "   ' Dietary Sources of Phosphorus  Table 10.4 Phosphorus Content of Various Foods  632  |  Phosphorus'],\n",
       "  'page_sentence_count_spacy': 6,\n",
       "  'sentence_chunks': [['Age Group  RDA (mg/day) UL (mg/day)  Infants (0–6 months)  100*  –  Infants (6–12 months)  275*  –  Children (1–3 years)  460  3,000  Children (4–8 years)  500  3,000  Children (9–13 years)  1,250  4,000  Adolescents (14–18 years)  1,250  4,000  Adults (19–70 years)  700  4,000  Adults (> 70 years)  700  3,000  * denotes Adequate Intake  Micronutrient Information Center: Phosphorus.',\n",
       "    'Oregon State  University, Linus Pauling Institute.',\n",
       "    'http:/ /lpi.oregonstate.edu/mic/ minerals/phosphorus.',\n",
       "    'Updated in July 2013.',\n",
       "    'Accessed October 22,  2017.',\n",
       "    ' Dietary Sources of Phosphorus  Table 10.4 Phosphorus Content of Various Foods  632  |  Phosphorus']],\n",
       "  'num_chunks': 1}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example:\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.59</td>\n",
       "      <td>198.89</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.15</td>\n",
       "      <td>10.32</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.44</td>\n",
       "      <td>95.75</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.11</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.75</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.69</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1232.50</td>\n",
       "      <td>215.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>308.12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1605.25</td>\n",
       "      <td>271.25</td>\n",
       "      <td>14.00</td>\n",
       "      <td>401.31</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.59           198.89                     9.97   \n",
       "std         348.86           560.44            95.75                     6.19   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.75           134.00                     4.00   \n",
       "50%         562.50          1232.50           215.00                    10.00   \n",
       "75%         864.25          1605.25           271.25                    14.00   \n",
       "max        1166.00          2308.00           429.00                    32.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
       "count           1208.00                    1208.00     1208.00  \n",
       "mean             287.15                      10.32        1.53  \n",
       "std              140.11                       6.30        0.64  \n",
       "min                0.00                       0.00        0.00  \n",
       "25%              190.69                       5.00        1.00  \n",
       "50%              308.12                      10.00        1.00  \n",
       "75%              401.31                      15.00        2.00  \n",
       "max              577.00                      28.00        3.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Stats:\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24723174c3634bd8b8a43acfb9e1b815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1843"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each chunk into its own item:\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts): \n",
    "    for sentence_chunk in item[\"sentence_chunks\"]: \n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "        # Join the sentences together into a paragraph-like structure, aka join the list of sentences into one paragraph\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" => \". A\" (will work for any captial letter)\n",
    "\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get some stats on our chunks\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 chars\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict) \n",
    "        \n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 489,\n",
       "  'sentence_chunk': 'Of course many behaviors are reflective of what we have easy access to—a concept we will discuss next. Societal Influences It is without a doubt that the American society affects what and how much we eat. Portion sizes have increased dramatically in the past few decades. For example, a bagel is now more than twice the size it was in the 1960s. Today, American teenagers have access to a massive amount of calorie-dense foods and beverages, which is a large contributor to the recent rapid increase in overweight and obesity in adolescents in this country. Even different cultures within the United States have different eating habits. For instance, Native Hawaiians and Pacific Islanders who have since adopted the western diet, post-colonization consume\\xa0 foods high in fat, which is a contributing factor to their higher incidences of overweight and obesity. The fast food industry in America not only supplies Americans with a large proportion of their diet, but because of its massive presence in society dominates the workings of the entire food system. To generalize, most fast food items have little nutritional merit as they are highly processed and rich in saturated fat, salt, and added sugars. Despite fast foods being a poor source of nourishment, Americans spend over one hundred billion dollars per year on fast food, up from six billion dollars in the early 1970s.',\n",
       "  'chunk_char_count': 1380,\n",
       "  'chunk_word_count': 226,\n",
       "  'chunk_token_count': 345.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample:\n",
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>583.38</td>\n",
       "      <td>734.83</td>\n",
       "      <td>112.72</td>\n",
       "      <td>183.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>347.79</td>\n",
       "      <td>447.43</td>\n",
       "      <td>71.07</td>\n",
       "      <td>111.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280.50</td>\n",
       "      <td>315.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.00</td>\n",
       "      <td>746.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>186.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>890.00</td>\n",
       "      <td>1118.50</td>\n",
       "      <td>173.00</td>\n",
       "      <td>279.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>1831.00</td>\n",
       "      <td>297.00</td>\n",
       "      <td>457.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      1843.00           1843.00           1843.00            1843.00\n",
       "mean        583.38            734.83            112.72             183.71\n",
       "std         347.79            447.43             71.07             111.86\n",
       "min         -41.00             12.00              3.00               3.00\n",
       "25%         280.50            315.00             45.00              78.75\n",
       "50%         586.00            746.00            114.00             186.50\n",
       "75%         890.00           1118.50            173.00             279.62\n",
       "max        1166.00           1831.00            297.00             457.75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats:\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
       "      <td>766</td>\n",
       "      <td>114</td>\n",
       "      <td>191.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
       "      <td>941</td>\n",
       "      <td>142</td>\n",
       "      <td>235.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -41                      Human Nutrition: 2020 Edition   \n",
       "1          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "2          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "3          -37  Contents Preface University of Hawai‘i at Māno...   \n",
       "4          -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \n",
       "0                29                 4               7.25  \n",
       "1               308                42              77.00  \n",
       "2               210                30              52.50  \n",
       "3               766               114             191.50  \n",
       "4               941               142             235.25  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter chunks of text for short chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 29.25 | Text: 2. Lacto-vegetarian. This type of vegetarian diet includes dairy products but not eggs. Lifestyles and Nutrition | 27\n",
      "Chunk token count: 9.75 | Text: Table 3.5 Salt Substitutes Sodium | 185\n",
      "Chunk token count: 28.75 | Text: American Journal of Clinical Dietary, Behavioral, and Physical Activity Recommendations for Weight Management | 509\n",
      "Chunk token count: 20.25 | Text: Published 2002. Accessed December 2, 2017. Pacific Based Dietary Guidelines | 761\n",
      "Chunk token count: 3.75 | Text: 806 | Pregnancy\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -39,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE',\n",
       "  'chunk_char_count': 308,\n",
       "  'chunk_word_count': 42,\n",
       "  'chunk_token_count': 77.0},\n",
       " {'page_number': -38,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition by University of Hawai‘i at Mānoa Food Science and Human Nutrition Program is licensed under a Creative Commons Attribution 4.0 International License, except where otherwise noted.',\n",
       "  'chunk_char_count': 210,\n",
       "  'chunk_word_count': 30,\n",
       "  'chunk_token_count': 52.5}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter our DataFrame for rows with under 30 tokens\n",
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 533,\n",
       "  'sentence_chunk': 'percent and other nonvertebral fractures by 23 percent.4 A reduction in fracture risk was not observed when people took vitamin D supplements at doses of 400 international units. Many other health benefits have been linked to higher intakes of vitamin D, from decreased cardiovascular disease to the prevention of infection. Furthermore, evidence from laboratory studies conducted in cells, tissues, and animals suggest vitamin D prevents the growth of certain cancers, blocks inflammatory pathways, reverses atherosclerosis, increases insulin secretion, and blocks viral and bacterial infection and many other things. Vitamin D deficiency has been linked to an increased risk for autoimmune diseases. Immune diseases, rheumatoid arthritis, multiple sclerosis, and Type 1 diabetes have been observed in populations with inadequate vitamin D levels. Additionally, vitamin D deficiency is linked to an increased incidence of hypertension. Until the results come out from the VITAL study, the bulk of scientific evidence touting other health benefits of vitamin D is from laboratory and observational studies and requires confirmation in clinical intervention studies. Vitamin D Toxicity Although vitamin D toxicity is rare, too much can cause high levels of calcium concentrations or hypercalcemia. Hypercalcemia can 4.\\xa0Bischoff-Ferrari, HA, et al. (',\n",
       "  'chunk_char_count': 1348,\n",
       "  'chunk_word_count': 194,\n",
       "  'chunk_token_count': 337.0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example:\n",
    "random.sample(pages_and_chunks_over_min_token_len, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding: \n",
    "- An embedding is a way to represent data, such as words or images, as numerical vectors in a multi-dimensional space. This representation captures relationships and similarities between different pieces of data, making it easier for algorithms to process and understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding our text chunks using Hugging Face Library:\n",
    "\n",
    "# It may take some time for downloading the model and loading it for the first time.\n",
    "# Subsequent runs will not require downloading again; the model will be loaded from the cache.\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=\"cpu\")    # Download from hugging face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Sentence Transformer library provides an easy way to create embeddings.\n",
      "Embedding: [-3.44286300e-02  2.95328330e-02 -2.33643539e-02  5.57257496e-02\n",
      " -2.19098590e-02 -6.47060527e-03  1.02848671e-02 -6.57803118e-02\n",
      "  2.29718108e-02 -2.61121094e-02  3.80420424e-02  5.61403073e-02\n",
      " -3.68746668e-02  1.52787855e-02  4.37020473e-02 -5.19723520e-02\n",
      "  4.89479750e-02  3.58107663e-03 -1.29750716e-02  3.54384189e-03\n",
      "  4.23262566e-02  3.52606587e-02  2.49402728e-02  2.99176984e-02\n",
      " -1.99382678e-02 -2.39752326e-02 -3.33366124e-03 -4.30450439e-02\n",
      "  5.72014190e-02 -1.32517805e-02 -3.54477949e-02 -1.13936048e-02\n",
      "  5.55561259e-02  3.61094647e-03  8.88527097e-07  1.14026899e-02\n",
      " -3.82230096e-02 -2.43546255e-03  1.51314037e-02 -1.32734815e-04\n",
      "  5.00659980e-02 -5.50876148e-02  1.73444897e-02  5.00959419e-02\n",
      " -3.75959203e-02 -1.04463520e-02  5.08322567e-02  1.24861356e-02\n",
      "  8.67377147e-02  4.64143082e-02 -2.10690200e-02 -3.90251614e-02\n",
      "  1.99694349e-03 -1.42345466e-02 -1.86794717e-02  2.82669496e-02\n",
      " -4.17522378e-02 -3.59938340e-03 -1.30826663e-02  3.91671509e-02\n",
      " -2.92913504e-02  4.63229939e-02  6.12681964e-03 -3.85694094e-02\n",
      "  6.66883811e-02 -1.42419059e-02 -6.49578199e-02  4.04555956e-03\n",
      " -3.01188184e-03  6.58981726e-02  7.93332048e-03 -2.52185501e-02\n",
      " -3.52043398e-02 -2.06171963e-02  3.65961678e-02  2.41233744e-02\n",
      " -4.23979498e-02 -2.37271469e-02  7.19451830e-02  3.18896957e-02\n",
      "  8.56765732e-03  5.01809120e-02  3.06092482e-02 -3.81284542e-02\n",
      " -3.40645015e-02 -4.32109758e-02  5.26518226e-02 -4.27095592e-02\n",
      " -3.30608077e-02 -2.80990042e-02 -8.63214862e-03  3.27507183e-02\n",
      "  3.98531556e-02 -2.80643422e-02  1.62000749e-02  1.64635666e-02\n",
      " -7.26301596e-02 -2.35976260e-02 -4.99445805e-03 -2.43375730e-02\n",
      "  6.63573518e-02  1.01700807e-02 -7.33340392e-03  1.01678139e-02\n",
      " -8.24312717e-02  5.06568737e-02  1.54307461e-03 -3.88789899e-03\n",
      " -2.68747229e-02 -2.70628016e-02 -4.45155539e-02 -9.93513409e-03\n",
      " -6.68739527e-02  1.33291725e-02 -1.38136670e-02 -7.60722235e-02\n",
      " -4.59486507e-02  5.82300946e-02 -3.41541544e-02  2.06777602e-02\n",
      "  5.75287966e-03  1.62674505e-02  1.88210290e-02  2.27290597e-02\n",
      " -7.16705807e-03  1.31421667e-02 -2.34097820e-02 -3.14957984e-02\n",
      " -1.90285165e-02  2.24106833e-02 -2.75777988e-02  5.19171469e-02\n",
      "  6.12935536e-02 -3.74242826e-03 -4.91759218e-02  4.43300558e-03\n",
      "  2.02150531e-02 -3.07915397e-02  3.44955251e-02  3.42498831e-02\n",
      " -7.23908888e-04  2.76552588e-02  8.57979583e-04 -4.65564132e-02\n",
      "  6.29548728e-02 -4.82680788e-03  1.12508731e-02 -4.57350910e-02\n",
      "  2.62186956e-02  2.94705834e-02 -4.70294207e-02  1.26509145e-01\n",
      "  1.09374337e-02 -3.88611890e-02 -6.29502907e-02 -2.97410544e-02\n",
      "  7.95471445e-02  5.15913554e-02  5.60819097e-02 -1.68494098e-02\n",
      "  4.59670313e-02  5.02447874e-05 -1.38017545e-02  1.84812713e-02\n",
      " -1.42345037e-02 -1.03216805e-02 -1.79740172e-02 -1.39032668e-02\n",
      "  3.46016837e-03  8.58673081e-02 -1.29174888e-02  7.49304295e-02\n",
      "  6.11052429e-03  1.45746805e-02  2.25934051e-02  6.90806955e-02\n",
      "  5.59253134e-02  5.44429012e-03  8.17072988e-02  3.58046442e-02\n",
      " -4.49424870e-02  2.65792888e-02 -4.30802815e-02  5.07438965e-02\n",
      "  1.58738252e-02  3.93476710e-02 -1.61547195e-02  7.93661699e-02\n",
      "  4.35209880e-03  1.11143906e-02 -2.07478274e-02 -3.64702009e-02\n",
      "  1.79305319e-02 -4.53404598e-02 -1.21981241e-02  4.42192331e-02\n",
      " -3.85862179e-02  4.27426770e-02 -2.27931960e-04 -6.03185296e-02\n",
      "  1.44639506e-03  3.66891436e-02 -2.83557514e-04  7.67654702e-02\n",
      "  4.95438352e-02  1.62606053e-02  2.26804148e-02 -3.24033760e-02\n",
      " -3.86327505e-02  7.82950297e-02  2.78975107e-02 -2.20212098e-02\n",
      " -2.95080170e-02 -7.43438527e-02  4.18462865e-02 -2.27422058e-03\n",
      "  9.15873609e-03 -2.64815744e-02 -1.25546474e-02  2.28404831e-02\n",
      "  6.82077482e-02 -3.07936184e-02  3.61646302e-02  3.87405083e-02\n",
      " -2.34642960e-02 -1.86181888e-02 -1.29825392e-04  2.78771520e-02\n",
      "  1.14772171e-02  3.79706249e-02 -1.60052124e-02  7.01069161e-02\n",
      "  6.32945895e-02 -3.99301425e-02 -4.42786328e-02  5.96374348e-02\n",
      " -2.23135538e-02 -7.57414028e-02  1.49408933e-02 -9.69460607e-02\n",
      "  2.42266376e-02  3.49745923e-03  1.57966290e-03 -1.62706263e-02\n",
      " -1.31779360e-02 -5.74737927e-03  8.57103840e-02 -4.42351848e-02\n",
      " -5.16821491e-03  3.88116725e-02  3.97148654e-02 -2.85581350e-02\n",
      " -1.36332121e-03  5.21435998e-02  1.33008640e-02 -1.98847428e-02\n",
      " -3.53314690e-02 -3.27012292e-03  3.66953947e-02 -2.43322924e-03\n",
      " -2.35368647e-02  2.37296019e-02 -4.22030361e-03 -2.85154469e-02\n",
      " -9.95515008e-03  2.10746322e-02 -2.99481638e-02 -4.54378016e-02\n",
      "  2.57442333e-03  2.40145978e-02 -9.36568342e-03  3.95623595e-03\n",
      "  2.72044055e-02  1.66734904e-02  3.04440055e-02 -5.11822291e-02\n",
      "  1.59915630e-02 -2.37715984e-04  2.27039158e-02  4.44445200e-02\n",
      " -6.36408553e-02 -3.83594148e-02 -4.00811955e-02  2.31166352e-02\n",
      " -2.28346381e-02  5.22318780e-02 -5.64015470e-02 -5.54197747e-03\n",
      " -3.16336006e-02 -8.85500945e-03  1.54491132e-02  1.30278887e-02\n",
      "  3.72903533e-02 -3.58290151e-02 -1.00853397e-02  1.23442691e-02\n",
      "  7.60784447e-02  3.69417891e-02 -9.44153406e-03  5.70524484e-02\n",
      "  5.23618460e-02 -1.17083248e-02 -1.56439431e-02  6.22280873e-03\n",
      " -1.06243435e-02  5.60463555e-02 -5.64209372e-03  5.41886128e-03\n",
      " -1.21085085e-02  1.23996781e-02  2.18708371e-03 -9.17425286e-03\n",
      " -1.89263709e-02  3.57194133e-02 -5.53159136e-03 -7.21394597e-03\n",
      " -6.24068780e-03 -4.90311766e-03 -3.01039517e-02 -2.73905415e-02\n",
      "  3.76886465e-02 -3.26217880e-04  3.61380838e-02 -5.17627001e-02\n",
      " -3.06852013e-02 -3.95019017e-02 -3.99090312e-02 -3.96709591e-02\n",
      "  6.23497106e-02 -7.13255210e-03  5.07607532e-04 -1.15799541e-02\n",
      " -7.76436972e-03 -3.30222622e-02  2.27072332e-02  4.39099073e-02\n",
      "  1.80926155e-02  1.33063933e-02  1.04865190e-02 -2.82455236e-02\n",
      " -1.57835223e-02 -2.82920823e-02 -2.41274480e-02 -4.84205298e-02\n",
      " -8.66246503e-03  2.37213746e-02  1.99321155e-02 -1.35324169e-02\n",
      " -2.93466151e-02  1.42494207e-02 -1.25415633e-02 -1.46438153e-02\n",
      " -1.33557152e-02  1.02691632e-02 -5.04000410e-02  6.88536912e-02\n",
      "  4.72585969e-02  1.26611814e-03 -1.89664047e-02 -2.18124315e-03\n",
      "  6.67325035e-02 -2.33003478e-02 -4.59404923e-02  1.23993191e-03\n",
      " -5.13819046e-02 -3.30483355e-02 -1.82183168e-03 -4.85388748e-02\n",
      "  2.42682304e-02  4.89649270e-03 -3.73755302e-03  9.66764148e-03\n",
      " -1.69690084e-02  8.71259645e-02  5.44295534e-02 -3.85265350e-02\n",
      "  3.44931595e-02 -3.72254737e-02  3.47704217e-02  3.20375268e-03\n",
      "  3.44035514e-02 -1.16729148e-01 -4.00709733e-02 -7.51275336e-03\n",
      " -3.64854820e-02  4.74884026e-02  6.11871341e-03  4.82693641e-03\n",
      " -9.75745991e-02  1.94991846e-02  2.06659548e-02  5.37244603e-02\n",
      " -3.83447669e-02 -1.50264436e-02 -4.93642576e-02  1.69349276e-02\n",
      " -1.32873738e-02 -3.25810462e-02 -1.35620311e-02  7.58092428e-05\n",
      " -5.32805622e-02 -6.10713065e-02 -1.14890859e-02 -3.04298215e-02\n",
      " -6.29046634e-02  3.11574414e-02 -4.25547594e-03  5.35519756e-02\n",
      "  5.80748252e-04 -3.18181664e-02 -7.51201361e-02 -2.28261854e-02\n",
      " -6.52606264e-02  2.64530983e-02  3.56256440e-02 -2.42591463e-02\n",
      " -2.48210747e-02  7.10836006e-03  3.44615802e-02 -3.58824544e-02\n",
      " -2.35385559e-02  2.89773047e-02  9.04023573e-02 -3.30274529e-03\n",
      "  9.67262778e-03 -4.82161194e-02  1.33224726e-02  2.13714391e-02\n",
      "  4.24260125e-02  2.80175265e-02 -1.16783837e-02 -8.53002362e-04\n",
      "  3.61301191e-03  4.91142600e-05 -4.21287715e-02 -3.83800231e-02\n",
      "  2.59826351e-02 -1.66465230e-02  4.53917645e-02 -8.60693157e-02\n",
      " -3.78084257e-02  5.70828877e-02  2.29125209e-02  3.66921760e-02\n",
      "  1.06494827e-02  1.90632492e-02 -1.82973663e-03 -5.97028434e-02\n",
      " -1.24359624e-02  1.07637746e-02  1.96661800e-02  2.29673162e-02\n",
      " -2.30668318e-02  5.48702553e-02  3.38971056e-02  1.75856762e-02\n",
      " -2.28890330e-02 -3.06942575e-02 -4.81055714e-02  7.13960221e-03\n",
      "  2.17535216e-02  8.45425110e-03  1.33396098e-02 -4.54222895e-02\n",
      "  3.38962935e-02  4.58740592e-02  3.46708409e-02 -7.65073299e-02\n",
      " -1.93071254e-02  9.84594505e-03 -1.36193531e-02  1.15063712e-02\n",
      "  3.78279351e-02 -6.25107344e-03 -3.61666968e-03 -6.28121793e-02\n",
      "  3.25885266e-02  2.00627446e-02 -6.75871074e-02  1.74651835e-02\n",
      " -5.44601269e-02 -2.69646756e-02 -3.39342915e-02  9.15184803e-03\n",
      " -6.85480796e-03  1.50181036e-02 -1.14632294e-01 -3.48747149e-02\n",
      " -5.34086823e-02  2.59111505e-02  4.46100868e-02  1.63094271e-02\n",
      "  8.70553870e-03 -2.26442665e-02 -2.66445521e-02  6.84185745e-03\n",
      "  1.11482013e-02  5.68693504e-02  4.96933684e-02 -1.27541479e-02\n",
      " -1.26163336e-02  2.45030094e-02 -3.40678245e-02 -1.06380628e-02\n",
      " -7.31094107e-02 -5.42687904e-03 -3.50816436e-02 -1.15438886e-02\n",
      " -2.00235285e-02 -1.32712601e-02 -4.83776145e-02  4.29345109e-02\n",
      " -2.42103375e-02  5.23328483e-02 -2.63345148e-02  3.02771037e-03\n",
      "  7.88600966e-02 -9.47292149e-03  5.09274229e-02 -6.98978966e-03\n",
      " -2.83143297e-02  6.09138682e-02  6.34606928e-02 -6.49071187e-02\n",
      " -1.86333675e-02  4.04768577e-03  1.80158429e-02 -3.19798104e-02\n",
      " -4.36901189e-02  2.82262657e-02 -2.02955399e-02 -1.79639377e-03\n",
      "  2.56272033e-02  6.55631814e-03 -5.57638034e-02 -1.39311953e-02\n",
      "  3.88334729e-02  3.77677679e-02 -3.07318810e-02 -1.68724041e-02\n",
      " -3.49365361e-02 -6.65668845e-02 -1.88083556e-02  3.86462510e-02\n",
      " -1.54486094e-02  9.48594511e-03 -1.73503458e-02  8.04667454e-03\n",
      "  1.75370127e-02 -2.30519939e-02 -1.32560479e-02  3.37193869e-02\n",
      " -1.54417418e-02 -6.87238723e-02 -3.28853056e-02 -4.49871239e-33\n",
      " -4.44906838e-02  9.54693649e-03 -2.57224403e-02 -3.21281664e-02\n",
      " -4.30240249e-03 -4.26217401e-03  5.16675971e-03  2.25371830e-02\n",
      "  2.49413140e-02  3.24431024e-02  9.34808888e-03 -2.73895059e-02\n",
      "  1.38996867e-02  1.00506768e-02 -1.26832817e-03  2.90706288e-02\n",
      "  2.60552298e-02 -1.68451220e-02  1.71560869e-02  1.25806034e-02\n",
      "  1.24600269e-02  3.60122919e-02  4.09958512e-02 -3.05389743e-02\n",
      " -2.42153909e-02  1.16511015e-02 -8.61147977e-03 -3.64992907e-03\n",
      "  3.25473621e-02  4.22551706e-02 -2.76540257e-02  2.32557114e-02\n",
      " -2.10831072e-02  4.25280556e-02 -1.60155576e-02  6.31459206e-02\n",
      " -4.40687835e-02 -5.13208006e-03  4.02368829e-02 -2.52309348e-02\n",
      " -2.02982668e-02 -4.01854925e-02 -3.42488252e-02 -4.86407056e-02\n",
      "  4.07062694e-02  1.19460179e-02  3.70717049e-02 -1.10959972e-03\n",
      " -1.08807068e-02  1.28650498e-02 -8.01075473e-02 -8.55970255e-04\n",
      "  8.65104143e-03  2.05037128e-02  2.53296196e-02 -8.17069504e-03\n",
      "  6.28589245e-04 -3.34682874e-03 -2.97050807e-03 -8.26584641e-03\n",
      " -3.67429331e-02  3.16993147e-02  2.49395370e-02 -4.95020039e-02\n",
      " -4.88927774e-02  6.75727874e-02 -3.79922837e-02 -3.87111455e-02\n",
      " -7.73422793e-03  4.97960486e-02  2.09200047e-02  3.35242227e-02\n",
      "  5.47627397e-02  6.31318614e-02 -1.58095565e-02  2.19913386e-02\n",
      " -1.67724825e-02 -1.54925184e-02  2.51924861e-02 -3.56684974e-03\n",
      "  3.17917243e-02  3.06093283e-02  2.60381866e-02 -1.52969817e-02\n",
      " -1.50790298e-02  1.22895986e-02  5.31404000e-03 -6.28142506e-02\n",
      "  3.58325131e-02  8.64117872e-03 -3.25661562e-02  9.51270387e-03\n",
      "  3.60868238e-02 -1.93802677e-02 -2.90085818e-03  2.36175060e-02\n",
      " -6.19427906e-03  2.53082421e-02 -9.00893938e-03  3.83066274e-02\n",
      "  7.40728574e-03 -3.88698094e-02 -7.62222160e-04 -1.19214226e-02\n",
      "  2.58010775e-02  1.50566958e-02  2.97986288e-02 -1.24886613e-02\n",
      " -7.63765648e-02  7.84251431e-04 -4.59649181e-03 -5.96700758e-02\n",
      " -4.99217361e-02  3.65763623e-03 -1.55270351e-02  2.41611693e-02\n",
      " -5.76891471e-03 -1.69782038e-03  3.52600329e-02 -1.41619965e-02\n",
      " -2.79698018e-02 -2.50186212e-02  4.09227461e-02  3.31035070e-02\n",
      " -2.88113225e-02 -1.83183309e-02 -4.28755954e-02  4.02017049e-02\n",
      "  4.59586009e-02 -6.20828047e-02  1.84501670e-02  1.13507826e-03\n",
      "  1.72663292e-07  2.86144372e-02  5.50687090e-02  4.52570505e-02\n",
      "  5.29323006e-03 -1.29606696e-02 -2.52388008e-02  1.48602594e-02\n",
      "  6.78400695e-02 -3.56873125e-03  1.65683329e-02  1.49407927e-02\n",
      " -1.64327174e-02  1.32763637e-02  2.84507219e-02 -1.37935326e-01\n",
      "  2.26399936e-02 -3.04838307e-02 -2.96503808e-02 -4.93083298e-02\n",
      " -1.45468945e-02  1.27221733e-01  1.06428333e-01  4.55490425e-02\n",
      "  4.17893752e-02 -1.05863614e-02 -2.93935463e-02  3.29429656e-02\n",
      "  1.14066936e-02 -3.86650823e-02 -2.50283610e-02 -2.86876932e-02\n",
      " -2.30519399e-02 -1.87885351e-02 -7.56754959e-03 -3.12078148e-02\n",
      "  2.19139904e-02  2.03541238e-02  1.14067656e-03 -5.82866371e-03\n",
      "  7.71835307e-03 -4.40828614e-02 -1.21653182e-02 -2.23600008e-02\n",
      " -2.46035382e-02  5.00243492e-02 -6.77434877e-02 -1.67534016e-02\n",
      " -4.29059267e-02 -3.12047154e-02  2.83358321e-02  6.48821518e-03\n",
      " -2.46221628e-02 -4.35099527e-02  1.45791061e-02  4.03499529e-02\n",
      "  4.33949055e-03 -1.14601050e-02 -7.68447518e-02 -4.44949865e-02\n",
      "  3.59259802e-03 -1.57844871e-02 -1.04557220e-02 -2.14964002e-02\n",
      " -3.05430461e-02  5.50825372e-02  6.99715572e-04  1.58236008e-02\n",
      "  1.89766321e-34  4.58320305e-02  1.93463098e-02  4.65045534e-02\n",
      "  6.44745678e-02  4.34259884e-02 -3.92567627e-02  1.20938654e-04\n",
      "  1.26640396e-02  1.90869365e-02 -6.96959570e-02 -3.12183127e-02]\n",
      "\n",
      "Sentence: Sentences can be embedded one by one or in a list.\n",
      "Embedding: [ 3.94506194e-02 -4.60195914e-02 -3.29465233e-02  2.72825696e-02\n",
      " -3.61808166e-02  3.57465893e-02  4.55458947e-02 -1.09203635e-02\n",
      "  3.95562425e-02 -2.60225311e-02  6.42183572e-02  2.12016106e-02\n",
      "  3.99725884e-02 -6.04055673e-02  2.74325665e-02 -7.52282217e-02\n",
      "  4.09870967e-02  8.16751737e-03  2.56218966e-02  1.11968890e-02\n",
      " -8.31801724e-03  1.95502993e-02 -1.46528371e-02  1.46337934e-02\n",
      "  2.22779475e-02  5.03179058e-03 -2.79730465e-02 -5.24364039e-03\n",
      "  2.35681478e-02 -7.86471516e-02 -2.56239437e-02 -5.34629077e-03\n",
      "  1.74736045e-03 -9.19249430e-02  1.69435555e-06  2.65378170e-02\n",
      " -2.27521136e-02 -3.18034329e-02 -5.45587167e-02  2.40477268e-02\n",
      " -3.12013365e-02 -3.47731798e-03  3.40134539e-02  1.93418972e-02\n",
      " -1.51144341e-02 -1.26163743e-03  6.19680323e-02  6.36472628e-02\n",
      "  2.88906768e-02  7.44322538e-02 -1.31940506e-02 -1.69802494e-02\n",
      " -2.43855771e-02 -3.20618041e-02  5.30784987e-02  7.06739165e-03\n",
      "  4.35357261e-03 -2.78002899e-02  3.58126289e-03  6.75943047e-02\n",
      "  1.14424070e-02  4.03575860e-02  5.52664045e-03 -2.91214343e-02\n",
      "  2.67779436e-02 -1.59772225e-02 -6.10939600e-02 -4.90749031e-02\n",
      " -3.12806629e-02  6.63172966e-03  6.04848713e-02 -8.52193497e-03\n",
      "  1.80662831e-03  1.19787338e-03  4.75631468e-02 -3.71573418e-02\n",
      " -5.38034253e-02  4.93517183e-02  5.34408027e-03  9.74624790e-03\n",
      " -6.11062907e-03  3.15139815e-02 -1.20278271e-02 -2.98326090e-02\n",
      " -1.70657635e-02 -3.83765274e-03  2.07908042e-02 -3.89088280e-02\n",
      " -3.99045683e-02 -3.09957694e-02 -3.52467038e-02 -4.35826108e-02\n",
      " -1.33233704e-02 -3.39080952e-02 -3.73939844e-03 -5.88062033e-03\n",
      " -5.20700552e-02 -3.49299647e-02  4.16680090e-02 -4.62911464e-02\n",
      "  4.75239344e-02 -4.08671517e-03 -1.76504422e-02 -7.97255337e-03\n",
      " -1.10379020e-02  6.79830229e-03  1.32745113e-02 -5.35473749e-02\n",
      " -9.04047489e-02  5.23857661e-02 -1.12107294e-02 -6.70006275e-02\n",
      " -4.79618125e-02  8.53535682e-02 -2.55563594e-02 -3.02220304e-02\n",
      " -7.13325366e-02  2.57715676e-02  1.00014564e-02  3.85844894e-02\n",
      " -9.34014171e-02 -8.47602077e-03  2.22694408e-02  7.02883601e-02\n",
      "  1.52111035e-02 -2.20443495e-02 -3.22776474e-02  2.85470635e-02\n",
      "  5.91878258e-02 -2.11898591e-02 -6.07739715e-03  3.99074145e-02\n",
      "  6.24967217e-02  2.07948172e-03 -3.55499014e-02  1.55528204e-03\n",
      "  5.86454980e-02 -7.02503044e-03  1.67638026e-02 -1.22546162e-02\n",
      " -3.16377915e-02 -1.88513026e-02 -4.07726318e-03 -1.49545735e-02\n",
      "  1.36235189e-02  1.08601749e-02  7.54653942e-03 -4.71822582e-02\n",
      "  4.82561104e-02  4.59333882e-02 -3.33714969e-02  4.32215855e-02\n",
      "  1.80385225e-02 -3.25417332e-02 -3.28202583e-02 -8.23770743e-03\n",
      "  1.35699779e-01  1.34014152e-02  8.18541367e-03  3.36027220e-02\n",
      "  3.36917900e-02 -2.11380678e-03  1.11447810e-03 -3.28610577e-02\n",
      " -5.54256840e-03 -2.86582820e-02 -1.44179445e-02  7.77836749e-03\n",
      " -1.63697498e-03  5.23462221e-02 -6.21748976e-02  7.61910528e-02\n",
      " -1.31988828e-03  1.02659464e-01  3.20158415e-02  9.65954438e-02\n",
      " -2.53450871e-02 -1.59761086e-02  2.35257447e-02  3.39029729e-02\n",
      "  6.42942861e-02  4.50348631e-02 -4.25531566e-02  1.18687078e-02\n",
      "  4.72524017e-02  9.58064571e-03  4.67109419e-02  3.52371158e-03\n",
      " -6.84242370e-03 -6.18194416e-02 -7.68528506e-02  3.66971605e-02\n",
      "  7.27883962e-05 -1.47776222e-02 -5.42816408e-02  6.76849973e-04\n",
      " -4.59973030e-02  3.40237468e-02 -4.48035309e-03 -7.04051703e-02\n",
      " -2.66993903e-02  5.19265048e-02  6.37334445e-03  8.18767548e-02\n",
      " -4.24824394e-02  3.05735543e-02  2.68434621e-02 -5.60146421e-02\n",
      "  1.05601382e-02  4.97399122e-02  3.51453722e-02  4.71563637e-02\n",
      " -1.77412741e-02  9.53975040e-03  1.34899793e-02 -3.80258868e-03\n",
      "  2.69160252e-02  3.17842118e-04 -4.88001518e-02  1.52728604e-02\n",
      "  3.56386276e-03  1.10028833e-02  1.82809364e-02  2.56793536e-02\n",
      "  5.70624620e-02 -3.82737629e-02 -2.20718402e-02  1.84372831e-02\n",
      "  1.39299012e-03  6.33676723e-03 -5.16251149e-03  1.95886213e-02\n",
      "  2.87943836e-02  1.03928177e-02 -5.76099306e-02  2.30747331e-02\n",
      " -4.33857366e-02 -6.59393221e-02  1.28544923e-02 -6.38925284e-02\n",
      "  4.30605561e-03 -2.37187855e-02  1.58825479e-02  1.29222944e-02\n",
      " -4.12547924e-02 -3.86699475e-02  6.36546314e-02  3.45706485e-06\n",
      " -8.49629007e-03  3.43008041e-02  1.91255752e-02 -1.45718851e-03\n",
      " -3.02515794e-02 -8.76662508e-03  4.08350043e-02  4.12850687e-03\n",
      " -5.52450307e-02  3.68653499e-02 -3.09521817e-02 -2.16217060e-03\n",
      " -2.53924285e-03 -3.39325257e-02 -2.67671421e-02  6.10456243e-03\n",
      "  3.22820130e-03  5.12547605e-03 -1.72513514e-03 -1.03540167e-01\n",
      " -5.28449789e-02  1.26473547e-03 -2.70888265e-02 -3.91258970e-02\n",
      " -3.43275419e-03  1.26331626e-02  2.04498414e-02 -1.15071132e-03\n",
      " -5.07162744e-03 -1.98181868e-02  3.36483568e-02  6.16528466e-02\n",
      "  6.06275722e-03 -6.91691637e-02 -5.54088838e-02 -1.80393085e-02\n",
      " -6.92366995e-03  3.26418839e-02  4.90450207e-03 -4.96721976e-02\n",
      " -1.38560298e-03  2.14454182e-03  2.02384237e-02 -1.21733956e-02\n",
      "  4.48048413e-02 -7.09418058e-02  6.40073745e-03  5.66105824e-03\n",
      "  7.99997151e-02  8.38833153e-02  3.84650081e-02  1.13146782e-01\n",
      " -1.19969901e-02 -2.38376725e-02  4.53927927e-02 -8.12379550e-03\n",
      " -2.29130848e-04  4.90679182e-02 -2.60968171e-02  5.36177494e-03\n",
      "  1.49014604e-03  3.74517925e-02  5.77094704e-02  2.33267024e-02\n",
      " -1.12191420e-02  2.51069665e-02 -1.48596698e-02 -8.46156757e-03\n",
      " -7.27730989e-02 -5.92735931e-02 -2.31561735e-02  1.97592992e-02\n",
      "  5.75864948e-02 -1.24196159e-02  2.45723911e-02 -3.85869555e-02\n",
      "  1.12911966e-02 -4.94906446e-03 -4.72136140e-02  2.47737747e-02\n",
      "  3.19250375e-02  2.02118736e-02  4.60990239e-03  3.16925012e-02\n",
      " -2.18356121e-02  1.54268909e-02 -1.25520146e-02  5.27300127e-02\n",
      "  2.74009965e-02  2.36027502e-02 -4.43898002e-03 -2.86318865e-02\n",
      " -1.81875825e-02 -3.64779644e-02 -2.48934217e-02 -7.28059933e-02\n",
      " -3.26464325e-03 -1.49086665e-03  5.82215050e-03  1.19633647e-02\n",
      " -8.18235427e-02 -2.36844905e-02  2.76351999e-02 -2.70508900e-02\n",
      " -4.01102044e-02 -6.08465495e-03 -5.30437343e-02  2.07984298e-02\n",
      "  3.47126871e-02 -2.53547523e-02 -1.02993920e-02 -3.63543294e-02\n",
      "  5.94719946e-02  1.45126302e-02 -4.81238663e-02  8.79047960e-02\n",
      "  1.82800572e-02  3.81575176e-03  3.33478232e-03 -2.33509354e-02\n",
      " -5.40298522e-02  4.19224724e-02  4.35114016e-05 -1.68721937e-02\n",
      "  6.34447613e-04  5.79451360e-02 -1.43354079e-02  2.29788404e-02\n",
      "  2.79557370e-02 -5.61306532e-03 -9.09449416e-04  1.64279398e-02\n",
      "  3.19038033e-02 -1.05184577e-01  2.38298997e-02 -1.25354296e-02\n",
      " -4.88626063e-02  1.28126591e-02  3.56031992e-02 -5.78170717e-02\n",
      " -6.65760711e-02  4.41986285e-02  2.01297901e-03  8.38589948e-03\n",
      " -4.23811972e-02 -4.51353937e-03 -7.48765469e-02  6.83994591e-02\n",
      "  1.00251343e-02  2.94613596e-02 -3.92392464e-02 -1.12553220e-02\n",
      "  2.79880501e-02 -5.52147813e-02  2.22328361e-02 -5.89618124e-02\n",
      "  1.47710349e-02  1.87821854e-02  5.59154106e-03  1.82993580e-02\n",
      " -6.24339022e-02 -3.99118615e-03  3.22625064e-03 -4.93065082e-02\n",
      " -1.94173306e-02  2.24287026e-02  2.29391027e-02 -2.20739935e-03\n",
      " -1.19603062e-02  1.43146971e-02  3.53296250e-02 -3.44811566e-02\n",
      " -3.83206680e-02  6.25753999e-02  4.66416068e-02 -2.89423913e-02\n",
      " -9.05482389e-04 -3.08143701e-02  1.75269619e-02  1.20397173e-02\n",
      "  7.37230945e-03  2.29606461e-02  1.48366485e-03 -1.23128677e-02\n",
      " -2.91464850e-02 -3.02032903e-02 -6.91954195e-02  4.36505536e-03\n",
      " -1.35427332e-02 -3.79707329e-02  2.86029242e-02 -6.50294125e-02\n",
      " -3.06761693e-02  1.45280501e-02  5.50787263e-02  2.16544047e-02\n",
      "  2.13010120e-03  1.02813607e-02  1.76272877e-02 -9.81414318e-03\n",
      " -3.92662473e-02  4.67814803e-02 -2.76333652e-02 -1.19481022e-02\n",
      " -1.31179253e-02 -5.81253227e-03  7.04512931e-03 -1.09557053e-02\n",
      " -9.03500170e-02 -9.44747254e-02 -1.94100272e-02  3.86917330e-02\n",
      "  2.10944004e-02  1.92263536e-02 -3.05454861e-02  3.20800096e-02\n",
      " -1.25736445e-02  4.29057330e-02  3.93685885e-02 -1.42695308e-02\n",
      "  3.07254791e-02  1.20590888e-02 -6.12073857e-03  4.94571403e-02\n",
      " -1.94699038e-03 -8.04700516e-03  2.19468810e-02 -2.82116104e-02\n",
      " -4.53274921e-02  4.31808084e-03 -5.42755798e-02  1.99365746e-02\n",
      "  2.12052977e-03  1.44002656e-03 -1.75995529e-02 -3.38714197e-02\n",
      "  8.49272683e-03  3.35035808e-02 -7.54015818e-02 -5.74961267e-02\n",
      " -2.53694002e-02  4.08971868e-02 -3.35615389e-02  2.52468456e-02\n",
      " -3.35380509e-02 -6.32562488e-03 -1.42695410e-02  4.75850813e-02\n",
      "  1.02259424e-02  2.97203697e-02  3.96144874e-02 -9.34810489e-02\n",
      " -1.89154856e-02  2.55827364e-02 -4.30834927e-02  2.04077531e-02\n",
      " -4.72003184e-02  1.45463366e-02 -1.75375084e-03  1.13528296e-02\n",
      "  1.15917288e-02  2.28660535e-02 -6.34508282e-02  1.47842309e-02\n",
      " -3.72624956e-02  2.10190695e-02  2.23517027e-02 -1.40325967e-02\n",
      " -1.24983001e-03 -4.56303209e-02  3.18165347e-02 -7.65234604e-03\n",
      " -8.48293479e-04  1.41215092e-02  6.35773689e-02 -3.99807543e-02\n",
      " -1.36847291e-02  2.23052353e-02  2.17968058e-02 -4.16497774e-02\n",
      " -6.43500388e-02  2.76488271e-02 -1.26892300e-02 -5.30624129e-02\n",
      "  2.26988383e-02  2.43420880e-02 -5.74823581e-02  1.33287050e-02\n",
      " -6.21904666e-03  3.61784548e-02  4.71230149e-02 -2.97927689e-02\n",
      "  1.52175575e-02 -5.54655381e-02  5.19218557e-02  1.01801055e-02\n",
      "  3.74246724e-02 -1.51000707e-03 -1.99339017e-02  3.24008465e-02\n",
      "  4.02577072e-02 -5.11782616e-02 -2.63885371e-02  2.81549301e-02\n",
      " -5.15224896e-02  1.09588113e-02  2.71724183e-02 -5.99141205e-33\n",
      " -4.61422317e-02 -4.23173495e-02 -1.67188160e-02  3.73254307e-02\n",
      " -4.61217500e-02 -2.15667132e-02 -1.65635552e-02  5.16205952e-02\n",
      "  2.91250776e-02  6.20496385e-02 -2.03374475e-02  2.79692411e-02\n",
      "  3.66977081e-02  2.21638046e-02  7.45072141e-02 -3.22245806e-03\n",
      "  2.77555548e-02  6.52084500e-03  4.98230103e-03 -4.55225930e-02\n",
      " -2.88831466e-03 -9.37117543e-03  5.39529137e-02  2.08806116e-02\n",
      "  3.83369885e-02 -1.57755092e-02 -2.27227919e-02 -1.31494505e-02\n",
      "  1.99728906e-02  3.26070264e-02 -2.48872787e-02  1.23165073e-02\n",
      " -1.51987402e-02  1.10159395e-02  9.44322348e-03  8.21157917e-02\n",
      "  9.64475423e-03 -5.08846045e-02  3.87344472e-02 -2.39818245e-02\n",
      " -3.33574265e-02 -6.19319789e-02 -1.11230128e-02 -5.58355935e-02\n",
      "  6.19737320e-02 -1.69197060e-02  4.15020213e-02 -2.09720619e-02\n",
      "  5.73130045e-03  8.57912190e-03 -5.86885214e-02 -1.12907961e-02\n",
      "  6.37832389e-04  6.87232465e-02  1.00054545e-02  3.54054831e-02\n",
      "  8.07905477e-03  1.76471453e-02 -2.19168123e-02  3.76118571e-02\n",
      " -3.89815401e-03  9.18030813e-02 -1.08761108e-02 -6.23476282e-02\n",
      " -1.35753294e-02  3.57578546e-02 -1.14120943e-02 -3.42902425e-03\n",
      " -3.46071310e-02  7.45991245e-02 -2.34830566e-02  6.77838698e-02\n",
      "  2.97479681e-03  7.37740993e-02 -1.56190721e-02 -8.05369671e-03\n",
      " -5.19049056e-02 -2.83664465e-02  5.06653171e-03 -1.04815783e-02\n",
      "  1.76374111e-02  1.79909263e-02  6.60757942e-04 -3.94584872e-02\n",
      " -1.35172596e-02 -8.75224452e-03  2.63499245e-02 -2.61225607e-02\n",
      "  5.23411855e-02 -2.09163744e-02 -2.31539980e-02 -4.42435872e-03\n",
      "  2.16243304e-02  1.97309311e-02 -3.69445980e-02  4.66765165e-02\n",
      " -1.30796498e-02 -4.01231897e-04 -8.66932981e-03  3.45299542e-02\n",
      " -5.18445522e-02 -2.96737794e-02 -1.53029254e-02  5.91483992e-03\n",
      "  3.60213518e-02  2.74790153e-02  2.82065012e-02  4.00680397e-03\n",
      " -9.97015908e-02 -8.29704851e-03  2.31552720e-02 -2.55014580e-02\n",
      " -2.81154122e-02  6.83589606e-03 -4.93259216e-03  3.62293608e-03\n",
      " -7.81383645e-03  7.15459436e-02 -2.38050483e-02 -6.59092814e-02\n",
      " -2.17578076e-02  1.96545497e-02  2.48006172e-03 -9.13048070e-03\n",
      " -1.21908765e-02 -4.54898505e-03 -2.05130205e-02  5.24952933e-02\n",
      "  3.82367335e-02 -2.03311089e-02  2.47481987e-02  3.57695110e-02\n",
      "  2.33143467e-07 -1.71385962e-03  2.28040200e-02  5.06219827e-02\n",
      "  4.86921743e-02  6.02175435e-03  2.46713147e-03  3.01764794e-02\n",
      " -1.33058242e-02 -2.90281493e-02  5.88517170e-03  3.55133563e-02\n",
      " -2.20334297e-03  1.92669854e-02  3.27016860e-02 -7.60592073e-02\n",
      "  3.08768526e-02 -4.25110348e-02 -6.23767637e-02 -3.17472368e-02\n",
      " -1.58934444e-02  8.95298123e-02  6.29931018e-02  3.53812017e-02\n",
      "  4.89307009e-02 -1.93265360e-02 -2.36166846e-02  1.39258225e-02\n",
      " -3.35590704e-03 -2.03021765e-02  1.25537934e-02 -3.32358852e-02\n",
      " -4.20750566e-02 -1.31272087e-02  1.11988792e-02 -1.84816420e-02\n",
      " -1.07769463e-02  3.37351188e-02  4.44298759e-02  3.82315367e-02\n",
      "  4.57887724e-02 -3.54073420e-02 -1.02399224e-02 -3.95739637e-02\n",
      " -3.64505202e-02  1.32280048e-02 -3.80342901e-02  6.11773599e-03\n",
      " -1.86848249e-02  1.58430710e-02 -7.96991307e-03  3.23966928e-02\n",
      "  3.23467813e-02  2.25449856e-02 -1.41449403e-02  5.76613192e-03\n",
      " -5.40468432e-02  2.36878358e-02 -4.38574031e-02 -8.39887373e-03\n",
      " -6.15581498e-03 -1.34777576e-02 -1.57500245e-02 -2.78316680e-02\n",
      "  7.60586560e-03  5.47017083e-02  1.94104027e-03 -4.60631736e-02\n",
      "  1.73852974e-34  1.89644359e-02 -3.54341120e-02  3.50972302e-02\n",
      "  6.22210130e-02 -1.22190500e-02 -6.77722367e-03 -5.48246764e-02\n",
      "  3.47667299e-02 -7.32362969e-03 -6.11831173e-02 -1.40541829e-02]\n",
      "\n",
      "Sentence: I like horses!\n",
      "Embedding: [-2.67144516e-02  1.31017968e-01 -3.86219397e-02  8.03857204e-03\n",
      "  2.97368709e-02  4.18423563e-02 -2.23039184e-02  2.32260954e-02\n",
      "  4.10415307e-02 -2.95187533e-02 -3.46263722e-02 -1.83776300e-02\n",
      " -4.16668765e-02 -3.50136571e-02 -2.38865521e-03 -3.45176645e-02\n",
      "  3.66863571e-02 -9.95052606e-03 -1.36514539e-02  3.32558006e-02\n",
      "  2.50876695e-02  4.92446348e-02 -4.98318747e-02 -3.70693393e-02\n",
      " -8.86961818e-03  1.86221395e-02 -3.02469544e-02 -8.31604972e-02\n",
      "  4.80911843e-02  5.41726723e-02 -4.70784232e-02 -3.58009674e-02\n",
      "  2.20624078e-02 -5.00785839e-03  1.42282158e-06 -1.15012098e-03\n",
      " -5.59794046e-02  3.81627679e-02  1.37144504e-02  2.09261123e-02\n",
      "  1.77286081e-02 -4.40464839e-02 -1.82883646e-02 -3.46980169e-02\n",
      "  1.40275145e-02  6.92683132e-03  3.91350202e-02  1.17347762e-02\n",
      " -8.34458042e-03  2.72798985e-02 -6.54870924e-03  1.99991278e-02\n",
      " -6.94846213e-02  3.48816952e-03  2.37389281e-02  3.51725519e-02\n",
      "  1.91240609e-02 -5.14322445e-02  2.78950427e-02  7.01103185e-04\n",
      " -1.24893920e-03 -7.77337700e-02  5.04702237e-03  7.82277714e-03\n",
      "  1.70377251e-02 -2.29385885e-04 -2.19660979e-02 -6.28654705e-03\n",
      "  8.83765612e-03  1.61731541e-02  4.74316301e-03  1.63856503e-02\n",
      "  1.98698267e-02  2.87378039e-02 -1.13958269e-02  1.21814059e-02\n",
      " -3.25761619e-03  7.47990385e-02  2.07087840e-03  2.13712594e-03\n",
      "  2.36541685e-03  6.81824796e-03  1.78419929e-02 -6.76717330e-03\n",
      " -1.13652255e-02  3.47163714e-02  3.99641506e-02  6.02479849e-04\n",
      "  2.50220038e-02  3.51954512e-02 -3.58676985e-02 -1.77029353e-02\n",
      "  4.12770621e-02  4.19989452e-02 -4.30353247e-02 -3.04895658e-02\n",
      "  1.79912318e-02  3.39752249e-02  5.67291602e-02 -3.72519121e-02\n",
      "  7.71263661e-03  2.32814904e-02 -3.23418379e-02 -2.01844051e-02\n",
      " -2.13816725e-02 -2.77470890e-02  1.32658910e-02 -3.97538953e-02\n",
      "  6.59546107e-02 -1.38212508e-02 -9.20053571e-03  8.39999504e-03\n",
      "  2.81347223e-02  4.15300205e-02 -5.64223807e-03 -4.35977289e-03\n",
      "  1.77122150e-02  2.02861037e-02 -1.31513244e-02  1.41240237e-02\n",
      " -4.01486978e-02 -6.16060868e-02  3.30247506e-02  4.27827314e-02\n",
      " -3.56697664e-02  8.99660680e-03 -4.70691100e-02  3.80768888e-02\n",
      " -4.82298173e-02 -4.26932201e-02 -7.96995219e-03  5.02463244e-03\n",
      "  2.38699280e-02 -4.13715653e-02  3.72231342e-02  5.26337288e-02\n",
      " -1.24458978e-02  5.02849522e-04  1.85009465e-02 -1.01251090e-02\n",
      "  3.01014297e-02  7.05216825e-03 -3.35082524e-02  7.10088434e-03\n",
      "  1.53239788e-02  9.82496701e-03  2.05332674e-02  1.51319569e-03\n",
      " -9.56340600e-03  3.18279676e-02  7.05033448e-03  2.75495090e-02\n",
      " -8.35546106e-02  7.22255383e-04 -3.07107102e-02 -2.28449958e-03\n",
      " -3.29813287e-02 -1.86590794e-02  2.09018365e-02  4.57937270e-02\n",
      "  5.31819612e-02  6.56177755e-03 -1.76376966e-03 -2.55267769e-02\n",
      " -3.00172456e-02 -6.81450590e-02  2.76180506e-02 -1.68421715e-02\n",
      "  2.72241142e-02 -3.77175771e-02  3.64565440e-02 -2.31930688e-02\n",
      " -3.39562111e-02 -2.45279968e-02  6.90309778e-02 -7.11448677e-03\n",
      "  9.94261801e-02  3.98893841e-02  2.19163094e-02 -4.04604860e-02\n",
      "  3.13975662e-02 -6.93094432e-02  3.09213679e-02  1.91144831e-02\n",
      "  4.83726785e-02  1.97971612e-02  3.27672921e-02  4.79416028e-02\n",
      " -2.59468914e-03  5.95350284e-03  1.48088839e-02 -1.33875050e-02\n",
      " -1.18511301e-02  1.80749167e-02 -2.51696189e-03  2.23802514e-02\n",
      "  2.03121230e-02 -5.51623926e-02  5.08648939e-02 -1.74926836e-02\n",
      "  1.03711160e-02  5.48304170e-02 -4.66480618e-03  4.06192392e-02\n",
      "  1.64949242e-02 -1.50219025e-03 -3.87123227e-02 -4.08391887e-03\n",
      "  2.24078242e-02 -2.66852919e-02  7.18928576e-02 -1.56481229e-02\n",
      "  2.15382073e-02 -3.24763241e-03  4.30515073e-02  5.54415733e-02\n",
      "  1.54590337e-02 -2.34077740e-02 -4.02784720e-02  1.09013177e-01\n",
      " -3.61078009e-02 -3.90355755e-03  9.66882408e-02  3.55334319e-02\n",
      " -2.49859672e-02  4.56151441e-02  1.12770582e-02  3.19718830e-02\n",
      " -8.21709447e-03 -1.45097086e-02 -1.38146933e-02  2.27993634e-03\n",
      "  5.81482723e-02 -1.65185600e-03  3.89424711e-03 -3.25093535e-03\n",
      "  4.48074229e-02  4.96553145e-02 -1.68224517e-02 -4.11297418e-02\n",
      " -1.91907340e-03 -9.49492771e-03  5.89222973e-03  4.09447998e-02\n",
      "  1.46513677e-03 -5.32164425e-02 -2.95555294e-02  1.46508981e-02\n",
      " -3.67850475e-02  1.48291597e-02 -5.90046681e-02 -2.52038632e-02\n",
      " -5.49021363e-02  2.71408688e-02 -2.80363671e-03 -3.59128788e-02\n",
      " -3.64186354e-02 -1.44418310e-02  1.95452757e-02 -6.66407645e-02\n",
      "  9.53727886e-02 -9.99759790e-03 -2.48823874e-02  6.06881380e-02\n",
      " -4.30383645e-02  4.59874608e-02  1.21795600e-02 -4.70494442e-02\n",
      " -1.12133482e-02  4.05425718e-03 -1.83668993e-02 -2.00734269e-02\n",
      "  8.09297338e-03  5.61417872e-03  2.90125073e-03 -4.12400486e-03\n",
      " -2.45109443e-02 -6.07830361e-02  3.15642580e-02  3.81212048e-02\n",
      " -2.57495176e-02 -9.00121592e-03  3.79892550e-02  3.19090784e-02\n",
      " -1.12440512e-02  1.84567329e-02 -1.08609572e-02  2.46243402e-02\n",
      "  2.10186057e-02 -5.99230193e-02 -1.32947890e-02  1.88184585e-02\n",
      " -9.76740010e-03  9.56682209e-03  2.00246722e-02 -2.43597068e-02\n",
      " -7.47997612e-02 -5.50076514e-02  2.16924632e-03 -1.73596945e-02\n",
      " -1.04215648e-02  2.91754846e-02  2.50064954e-02  2.84176283e-02\n",
      "  3.87749970e-02  1.19252186e-02  3.72133441e-02  3.90469804e-02\n",
      " -1.65001303e-02  6.95834011e-02 -1.63944196e-02 -7.37079158e-02\n",
      " -3.43169011e-02  2.05581891e-03  9.86350849e-02  5.81433512e-02\n",
      " -2.93991901e-02 -3.82711105e-02  3.70079018e-02  4.88188267e-02\n",
      " -1.09365135e-02  2.00260971e-02  3.07365954e-02  3.93032469e-02\n",
      " -5.45122176e-02  9.67298727e-03  3.63564081e-02 -8.13301746e-03\n",
      "  3.40572074e-02 -2.40374319e-02  1.62690040e-02 -6.15117187e-03\n",
      "  6.18251190e-02  1.78101715e-02  2.62790844e-02 -4.31745313e-02\n",
      "  2.31975298e-02  4.30519804e-02 -1.22824963e-02  2.35074013e-03\n",
      "  1.26593458e-02 -2.33552922e-02 -5.29602692e-02 -4.72016037e-02\n",
      " -4.13431749e-02 -3.42140235e-02 -6.31477982e-02 -5.49499281e-02\n",
      " -4.36866982e-03 -3.53800617e-02 -1.97227746e-02 -7.51119992e-03\n",
      "  2.12547765e-03  4.76517454e-02  1.09922430e-02  3.04179247e-02\n",
      "  1.46337692e-02  3.07517871e-02 -1.45270126e-02 -1.70470476e-02\n",
      "  3.67471538e-02  4.42354828e-02  3.12824622e-02 -2.07042824e-02\n",
      " -3.52868512e-02 -2.42242105e-02  1.73001830e-02 -5.09645939e-02\n",
      " -5.22649921e-02 -6.42453786e-03 -2.20864546e-02 -2.42289137e-02\n",
      " -1.85502563e-02  2.09545735e-02 -2.66731437e-02  7.72047341e-02\n",
      "  3.55064906e-02  4.87249456e-02  6.55237511e-02 -1.83372907e-02\n",
      "  4.43752632e-02 -1.96081623e-02 -4.16709622e-03  1.72038972e-02\n",
      " -7.87352547e-02  1.13609936e-02  3.29830721e-02 -3.34833488e-02\n",
      " -5.91834038e-02 -5.16828708e-03  3.64363156e-02  5.87292947e-03\n",
      " -6.47325590e-02 -6.63173869e-02  2.19204668e-02 -1.38641689e-02\n",
      " -7.08775669e-02  3.66111994e-02  1.24158468e-02  1.85580589e-02\n",
      " -9.74495430e-03  7.25327805e-02  3.39582004e-02 -3.37731801e-02\n",
      " -1.80621780e-02  1.93781164e-02  2.81449631e-02  5.28942095e-03\n",
      "  4.20942307e-02 -1.23083293e-01 -7.94750676e-02  9.75957606e-03\n",
      " -2.79009659e-02  1.05111813e-02  3.45771550e-03 -1.75463278e-02\n",
      "  1.61945336e-02 -3.45510691e-02  4.52590967e-03 -2.37182667e-03\n",
      " -1.51141249e-02 -2.47375406e-02 -1.73723530e-02 -2.17887014e-02\n",
      " -3.81727777e-02 -2.62229405e-02  5.28297611e-02  4.99212854e-02\n",
      " -7.42874295e-03 -9.25566535e-04 -3.52285616e-02  7.01323384e-03\n",
      "  2.04047393e-02 -2.87854653e-02 -6.26717657e-02 -3.56784947e-02\n",
      " -9.65091772e-03  3.77316624e-02 -2.32007150e-02 -5.49368039e-02\n",
      " -4.15531918e-02  4.60585468e-02 -2.85466127e-02 -1.02311904e-02\n",
      "  4.15115356e-02  4.87313718e-02 -5.12896851e-02 -2.63645574e-02\n",
      "  4.78796661e-02  4.24948754e-03 -1.24509800e-02  1.37372520e-02\n",
      " -2.45817117e-02  3.30335237e-02 -3.50699993e-03 -5.78273227e-03\n",
      "  5.48207853e-03 -7.99308419e-02  1.17996652e-02  2.62448844e-02\n",
      "  7.02878237e-02  3.68168810e-03 -2.48584952e-02  1.60750560e-02\n",
      "  9.65146697e-04  1.82267036e-02  9.39315744e-03 -2.96558850e-02\n",
      "  3.92309343e-03 -5.14191054e-02 -1.69606190e-02 -1.98429041e-02\n",
      " -3.49021028e-03  3.12085375e-02  3.27818021e-02  5.27704209e-02\n",
      "  4.46242280e-03  1.18340757e-02  2.50518695e-02 -4.76561971e-02\n",
      " -7.44548067e-02  5.99202141e-02 -5.69956377e-03 -1.62266346e-03\n",
      " -1.20002788e-03 -3.23642828e-02  3.44793824e-03 -2.37544496e-02\n",
      "  4.43041744e-03 -3.08273342e-02 -3.59355621e-02  4.43713926e-03\n",
      " -4.75262776e-02 -8.31952468e-02  4.26745554e-03  6.36712536e-02\n",
      "  2.90642511e-02 -6.99088871e-02  1.03362063e-02  1.88462529e-02\n",
      " -3.32860760e-02  3.96924950e-02 -9.97587945e-03  6.79315850e-02\n",
      "  2.69873831e-02 -3.28908395e-03 -1.17854970e-02  3.25983856e-04\n",
      " -1.81754790e-02 -2.56635877e-03  1.76420622e-02 -7.90652540e-03\n",
      " -4.59221117e-02 -1.44926067e-02  7.05569088e-02  7.63443932e-02\n",
      " -2.13224888e-02 -7.01023564e-02 -2.44030431e-02 -4.42189947e-02\n",
      " -1.92245245e-02 -3.43547110e-03 -5.37870824e-03  1.60381496e-02\n",
      "  4.31605708e-03  2.63602361e-02  9.25269164e-03 -5.00051342e-02\n",
      " -2.23820619e-02  2.37112306e-02 -3.18208039e-02  1.91109872e-03\n",
      "  2.22122800e-02 -2.86415201e-02 -3.01541369e-02  4.24447693e-02\n",
      "  3.16200741e-02 -4.11088951e-03 -1.94385312e-02  4.66424637e-02\n",
      " -2.17027217e-02 -2.05982081e-03  2.94458568e-02 -8.11043158e-02\n",
      " -3.00531108e-02  8.84201471e-03  5.94104752e-02 -4.32767197e-02\n",
      " -3.30853276e-02 -3.78850549e-02 -7.49539165e-03 -2.94030830e-02\n",
      " -2.78828591e-02 -6.97876960e-02 -1.04949608e-01 -6.08098693e-33\n",
      " -5.51380999e-02  5.61124878e-03 -2.12247372e-02 -1.64441355e-02\n",
      " -1.91945881e-02 -6.31383732e-02  7.29616433e-02  5.43050878e-02\n",
      "  9.38583352e-03  5.87568581e-02 -4.03266177e-02 -1.17488811e-02\n",
      " -3.14355060e-03  3.06387222e-03  1.02370242e-02  9.91448760e-03\n",
      "  4.25155349e-02  1.35757737e-02 -1.42614963e-03 -1.04992818e-02\n",
      "  1.34479925e-02  1.84357595e-02  2.05373112e-02 -2.03614719e-02\n",
      "  5.92006706e-02  4.36169505e-02 -1.34567367e-02 -2.44422480e-02\n",
      "  1.97254140e-02  7.98829272e-02 -2.71733361e-03 -2.49470882e-02\n",
      " -5.54749509e-03 -5.11282384e-02 -1.14621781e-02  7.02913031e-02\n",
      " -3.59583497e-02  6.75166864e-03  7.97247235e-03  2.47955453e-02\n",
      "  8.60149711e-02 -6.06778152e-02  5.15737087e-02 -8.29937384e-02\n",
      " -6.10619877e-03  5.53287752e-03  3.96847576e-02  4.83613312e-02\n",
      "  5.62811047e-02  3.28994617e-02 -3.95440087e-02 -6.64222687e-02\n",
      " -6.01476012e-03 -2.22541895e-02  4.40150835e-02 -1.99889503e-02\n",
      "  4.15038653e-02  4.01953887e-03  6.46273838e-03 -7.56389415e-03\n",
      " -2.00565886e-02 -4.12345827e-02 -5.27454279e-02  1.99447740e-02\n",
      " -2.42805239e-02  9.00406763e-02 -1.36347569e-03 -2.23540906e-02\n",
      "  1.24890218e-03  2.67293397e-02 -4.62033525e-02 -4.11433959e-03\n",
      "  1.04217026e-02 -2.03547720e-02 -3.05201183e-03  2.12044194e-02\n",
      " -2.47737877e-02 -2.73370016e-02 -7.42391497e-02  2.22837273e-02\n",
      "  1.23512316e-02 -1.80122722e-03 -1.29682217e-02  2.24856660e-02\n",
      "  2.74916291e-02  4.72554527e-02  1.47988675e-02 -7.57994782e-03\n",
      " -6.21416122e-02 -1.62536819e-02  2.41620280e-02  1.02008665e-02\n",
      " -1.31030865e-02 -8.06463603e-03 -2.23883055e-02 -8.22523143e-03\n",
      " -2.31066439e-02 -4.72937413e-02 -9.15289205e-03 -8.02163128e-03\n",
      "  6.06632568e-02 -5.54484827e-03 -1.17456215e-02  1.09999711e-02\n",
      "  1.48121184e-02  7.78211979e-03  1.76356193e-02 -2.45958827e-02\n",
      " -6.22097589e-02  4.78604101e-02  4.24484140e-04  4.90715541e-02\n",
      "  4.79640029e-02  1.08700074e-01  4.72227894e-02 -2.13420279e-02\n",
      "  1.43255070e-02  5.01378328e-02 -5.37697971e-02 -6.70107873e-03\n",
      "  5.06178010e-03 -1.76747832e-02 -8.18694085e-02  1.33880763e-03\n",
      "  9.54762567e-03 -2.69555654e-02 -8.88636522e-03 -6.27400726e-02\n",
      " -3.88119295e-02 -2.61610113e-02  1.80818047e-02 -1.85252577e-02\n",
      "  2.16867434e-07 -3.05570774e-02 -1.17482757e-02  3.86133566e-02\n",
      "  3.74167114e-02  2.54942048e-02  5.64584434e-02 -1.09358951e-02\n",
      " -3.30961868e-03  1.25073427e-02  9.65328068e-02 -1.43617541e-02\n",
      "  3.24686617e-02  3.97201926e-02 -4.94120829e-03 -2.60988646e-03\n",
      " -6.81308582e-02 -1.66515987e-02 -2.99731735e-02  3.99055053e-03\n",
      " -8.13999400e-03 -9.24562514e-02 -2.10931282e-02 -1.69722252e-02\n",
      " -2.74126139e-03 -1.28148878e-02  2.96035782e-02 -6.09628565e-04\n",
      " -3.85000296e-02  4.86925943e-03  3.31277661e-02 -6.07116222e-02\n",
      "  3.97328474e-02 -2.65968833e-02  1.81749631e-02  2.81270109e-02\n",
      " -1.95649043e-02 -1.59012470e-02 -2.35990938e-02  1.40775600e-02\n",
      "  6.34064451e-02 -3.07723582e-02  1.13034442e-01 -2.16423925e-02\n",
      "  4.25004028e-02  8.88848584e-03  5.16530387e-02  1.31784077e-03\n",
      " -1.07091129e-01  4.95355278e-02 -1.30839143e-02 -2.18527988e-02\n",
      " -2.33550090e-03  2.22141836e-02 -8.58236919e-04  7.25461636e-03\n",
      " -2.40625930e-03  4.42726165e-03 -4.03266884e-02  8.73129070e-03\n",
      " -4.62856069e-02 -9.82049946e-03  7.88558833e-03 -5.59528498e-03\n",
      "  4.54911180e-02 -8.76697339e-03 -4.82472479e-02 -1.01598747e-01\n",
      "  1.12966350e-34  1.71271861e-02 -4.39227968e-02 -3.20055522e-02\n",
      "  2.03909501e-02  2.53923126e-02 -1.92606263e-02  5.61988428e-02\n",
      " -2.65362673e-02  6.99775741e-02  1.96202453e-02 -2.20129229e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of sentences\n",
    "sentences = [\"The Sentence Transformer library provides an easy way to create embeddings.\",\n",
    "             \"Sentences can be embedded one by one or in a list.\",\n",
    "             \"I like horses!\"]\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape  # Represent the Each Sentence with 768 Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.45474076e-02,  7.66726956e-02, -2.85872333e-02, -3.31282988e-02,\n",
       "        3.65210623e-02,  4.78570424e-02, -7.08108172e-02,  1.62834208e-02,\n",
       "        1.93443522e-02, -2.80482415e-02, -2.91747209e-02,  5.11309542e-02,\n",
       "       -3.28720175e-02, -8.98753665e-03, -1.03672855e-02, -3.15488651e-02,\n",
       "        4.22783680e-02, -9.13287606e-03, -1.94017403e-02,  4.35689427e-02,\n",
       "       -2.31998023e-02,  4.29883264e-02, -1.72393285e-02, -2.01372374e-02,\n",
       "       -3.13574187e-02,  8.08164012e-03, -2.06725132e-02, -2.27869693e-02,\n",
       "        2.44812425e-02,  1.71968229e-02, -6.26672804e-02, -7.54797310e-02,\n",
       "        3.57421711e-02, -5.46572637e-03,  1.24730320e-06, -7.63199665e-03,\n",
       "       -3.53221931e-02,  1.91326831e-02,  3.99046019e-02,  2.11734464e-03,\n",
       "        1.64565891e-02,  9.84057132e-03, -1.80700831e-02,  9.33838170e-03,\n",
       "        3.23482789e-02,  5.84784821e-02,  4.23187055e-02,  1.62091162e-02,\n",
       "       -9.14910734e-02,  1.82305351e-02, -5.25728147e-03, -7.81020615e-03,\n",
       "       -3.47644202e-02, -1.01791476e-04, -9.65474918e-03,  5.22900224e-02,\n",
       "        1.52456313e-02,  1.05422381e-02,  2.10148301e-02, -1.38553157e-02,\n",
       "       -3.75944003e-02, -5.72233945e-02,  6.64911866e-02,  2.45623849e-03,\n",
       "        2.55166702e-02,  3.77774984e-02,  2.76863798e-02, -2.45716665e-02,\n",
       "        2.43620872e-02,  3.49905565e-02, -3.49027850e-02, -4.93868208e-03,\n",
       "        6.81600906e-03,  4.50944491e-02,  6.90900814e-03,  1.14738084e-02,\n",
       "        3.66865024e-02,  4.02682535e-02,  4.24677543e-02,  1.21451197e-02,\n",
       "        4.90803923e-03,  6.70209248e-03, -3.07982396e-02, -4.74797154e-04,\n",
       "       -3.96070480e-02, -1.18890842e-02,  3.48101519e-02,  1.52127817e-02,\n",
       "        4.83171316e-03,  5.28494455e-02, -6.46775365e-02,  3.00130080e-02,\n",
       "        3.83021869e-02,  4.23714966e-02, -5.32575622e-02, -5.55911520e-03,\n",
       "       -2.74476707e-02, -2.95134149e-02,  4.75043803e-02, -1.10218758e-02,\n",
       "       -5.88265024e-02, -1.16618015e-02, -8.33427235e-02, -2.46357080e-02,\n",
       "        4.74454574e-02,  5.46443500e-02, -2.42980886e-02, -5.29453307e-02,\n",
       "        2.28929576e-02, -8.70030606e-04, -4.88942601e-02, -2.53103152e-02,\n",
       "        7.70678073e-02,  5.69900051e-02,  2.86229663e-02, -1.57336914e-03,\n",
       "        7.94348195e-02,  2.24736296e-02,  5.00359712e-03, -3.96804186e-03,\n",
       "        8.58052820e-03, -2.52369195e-02, -8.24427232e-03,  5.32644652e-02,\n",
       "        6.59034261e-03,  1.82210617e-02, -8.69629309e-02,  4.31967974e-02,\n",
       "       -2.86612362e-02, -5.18719330e-02, -9.11247509e-04, -9.71054379e-03,\n",
       "       -3.46385241e-02, -2.71559302e-02,  3.32040191e-02, -2.43676212e-02,\n",
       "       -1.89912654e-02, -4.16371822e-02,  6.64050058e-02,  9.75431816e-04,\n",
       "       -6.71448710e-04,  1.23330776e-03, -1.88144501e-02,  4.68906984e-02,\n",
       "        5.46860322e-02,  8.01841076e-03,  1.75472330e-02, -2.19881954e-03,\n",
       "        5.00558596e-03,  5.67321926e-02,  6.96682706e-02,  1.46436915e-02,\n",
       "       -9.31071490e-02, -1.26185687e-02, -7.23036602e-02,  1.16096708e-04,\n",
       "        5.40898852e-02, -3.32767293e-02,  2.48239283e-02,  6.72468469e-02,\n",
       "        3.45424041e-02, -4.99136699e-03, -4.48252521e-02, -5.55035509e-02,\n",
       "       -5.55662699e-02, -4.72544618e-02,  5.21704089e-03,  1.99847389e-02,\n",
       "        3.52876703e-03, -7.52701312e-02,  2.67593507e-02,  3.13316323e-02,\n",
       "       -2.13878453e-02, -2.53582913e-02,  6.60627410e-02, -3.79814743e-03,\n",
       "        1.72973098e-03,  2.55590118e-02, -4.31265645e-02,  4.05430980e-03,\n",
       "        1.85788125e-02,  1.59321986e-02,  1.00603383e-02,  3.54764424e-02,\n",
       "        2.57358840e-03,  1.46150663e-02,  1.08087657e-03,  6.17927015e-02,\n",
       "        1.60133168e-02,  1.55217852e-02,  3.38939577e-03,  7.38757942e-03,\n",
       "        4.62787738e-03,  1.19771259e-02, -2.22111661e-02,  3.32431532e-02,\n",
       "        1.30507182e-02, -9.45958570e-02,  1.92044303e-02, -2.13419273e-02,\n",
       "       -5.28250262e-03,  9.01520252e-02,  4.75887209e-02, -2.46056914e-02,\n",
       "        6.42947736e-04,  5.84885711e-03, -4.05396298e-02, -1.55255478e-02,\n",
       "        3.68703716e-02,  2.34616380e-02,  5.74560277e-02, -2.46109813e-02,\n",
       "       -2.94354334e-02, -7.99209159e-03,  1.55303599e-02, -3.91083397e-03,\n",
       "        1.78512204e-02, -1.75801397e-03, -1.83040556e-02,  9.47188288e-02,\n",
       "       -2.54314486e-02, -3.77975032e-02,  9.79456306e-02,  1.79755390e-02,\n",
       "       -7.73815438e-03,  2.27807108e-02,  4.57149185e-02,  1.71551798e-02,\n",
       "        3.51931117e-02, -1.90126169e-02,  1.39740505e-03,  3.31789032e-02,\n",
       "        4.44040410e-02,  2.86699478e-02,  5.34522645e-02,  3.28312479e-02,\n",
       "        5.52565325e-03,  2.23779958e-02, -9.38726496e-03, -6.68331981e-02,\n",
       "        2.99007278e-02,  6.89629614e-02, -1.36478767e-02,  2.98763458e-02,\n",
       "       -1.83594991e-02, -6.84988946e-02,  7.55537674e-03, -7.85366457e-04,\n",
       "        1.22811720e-02,  8.88288487e-03, -3.46379653e-02,  1.12969456e-02,\n",
       "       -4.44449000e-02,  1.00413617e-02, -3.62991802e-02, -2.39463206e-02,\n",
       "        2.97464933e-02, -2.73257438e-02,  1.05879428e-02,  1.13884583e-02,\n",
       "        7.53291249e-02, -1.48294931e-02, -6.36837333e-02,  4.08296213e-02,\n",
       "       -5.22071458e-02, -8.53456324e-04, -3.06047518e-02,  4.63196486e-02,\n",
       "       -9.17660538e-03,  3.12782801e-03,  3.91602218e-02, -3.47800180e-02,\n",
       "       -2.86343805e-02,  4.81524728e-02, -2.93691782e-03,  1.97712556e-02,\n",
       "        1.21127283e-02, -6.82448596e-02,  1.90935321e-02,  1.09582618e-02,\n",
       "       -6.06389865e-02,  3.87973827e-03,  3.91351804e-02,  4.65689935e-02,\n",
       "        4.67327423e-03,  5.19058034e-02,  2.15037819e-02, -1.10510131e-02,\n",
       "        4.28912081e-02,  5.92603767e-03, -1.54299848e-02, -1.17191253e-02,\n",
       "        1.04201809e-02,  2.26957221e-02,  5.02244849e-03,  8.50964803e-03,\n",
       "       -3.36886942e-02, -5.01523130e-02,  5.59586100e-03, -5.59638962e-02,\n",
       "        3.78462439e-03, -1.90141480e-02,  8.82788934e-03,  1.77502241e-02,\n",
       "        5.09780124e-02, -2.01751105e-02,  2.21496611e-03,  9.00214165e-02,\n",
       "        9.43100546e-03,  1.28152175e-02, -5.16687371e-02, -6.71980157e-02,\n",
       "       -6.65828586e-03,  4.59541008e-02,  6.21871687e-02,  6.29634038e-02,\n",
       "       -7.76772276e-02, -6.27348721e-02,  2.13560760e-02, -3.54639143e-02,\n",
       "       -2.72188261e-02, -1.83704011e-02,  2.57109702e-02,  2.73733772e-02,\n",
       "       -2.05341820e-02,  3.12488675e-02,  6.17937520e-02,  2.82254373e-03,\n",
       "        9.42654535e-02,  5.39666566e-04, -3.67932990e-02,  2.16332097e-02,\n",
       "        5.56003489e-02,  8.22860561e-03,  1.73951816e-02, -6.19068444e-02,\n",
       "        1.15494812e-02, -6.15611300e-02, -1.46468577e-03,  1.73469689e-02,\n",
       "       -7.23905256e-03, -4.79254201e-02, -2.79301517e-02,  3.57203558e-02,\n",
       "       -5.44241909e-03,  2.41938308e-02,  9.67005081e-03, -4.45232093e-02,\n",
       "       -5.65502420e-02, -7.03274906e-02, -5.13420394e-03, -1.22374155e-01,\n",
       "        4.00631614e-02,  7.84556661e-03,  1.76967513e-02,  1.78066567e-02,\n",
       "        1.19575607e-02,  2.81822905e-02, -8.97248369e-03,  2.16233153e-02,\n",
       "        2.00863574e-02,  2.75822747e-02, -5.10546193e-03,  7.55687477e-03,\n",
       "       -2.15563625e-02,  5.08302562e-02,  3.42583866e-03, -5.21124005e-02,\n",
       "       -5.57864644e-02,  1.04195392e-02,  4.05402482e-03, -3.89529951e-02,\n",
       "       -7.80597003e-03,  1.77253224e-02, -1.63437035e-02,  3.43393944e-02,\n",
       "        6.19499683e-02,  2.41787843e-02,  7.99908582e-03,  1.24411471e-02,\n",
       "        2.85640191e-02,  2.50759833e-02,  1.47869224e-02,  6.14376627e-02,\n",
       "       -5.95356189e-02,  5.64276299e-04,  6.96323905e-03, -9.42605734e-03,\n",
       "       -2.98646819e-02, -1.79769024e-02,  3.68007421e-02, -7.61808157e-02,\n",
       "       -2.57906802e-02, -8.80654976e-02, -1.99168194e-02, -3.44833620e-02,\n",
       "       -3.03549599e-02,  3.44826244e-02, -7.38917897e-03, -1.73989055e-03,\n",
       "       -2.12657172e-02,  5.19678295e-02,  7.11391494e-02,  1.89423058e-02,\n",
       "        6.76491996e-03, -6.69399202e-02,  3.58918272e-02, -1.40842935e-03,\n",
       "        6.70214593e-02, -1.10296629e-01, -1.04557700e-01,  1.18081113e-02,\n",
       "       -1.29538607e-02, -1.38785923e-02, -4.69234437e-02, -5.10462709e-02,\n",
       "        1.18376696e-02, -1.15415349e-03, -1.65975876e-02,  1.63002908e-02,\n",
       "       -3.08757387e-02, -4.04602997e-02,  1.09036956e-02,  1.91065588e-03,\n",
       "       -1.86016560e-02,  4.51204069e-02, -3.49305943e-02,  4.07409556e-02,\n",
       "       -2.27419697e-02,  4.51251864e-02,  1.25924181e-02, -6.06238134e-02,\n",
       "        3.06807458e-02, -8.21872354e-02, -3.94097976e-02, -1.80607177e-02,\n",
       "       -3.17829922e-02, -1.87635515e-02, -6.69391304e-02, -2.84681004e-02,\n",
       "       -3.36956233e-02,  7.08523169e-02, -4.08128239e-02,  1.86223108e-02,\n",
       "        3.72806005e-02,  5.44050597e-02, -3.62138227e-02, -2.40352135e-02,\n",
       "        3.33079733e-02,  1.48777366e-02,  3.82788712e-03,  2.03329716e-02,\n",
       "       -1.90318003e-02,  6.85469434e-02, -2.61596385e-02,  7.61797046e-03,\n",
       "        7.72189489e-03, -2.78083775e-02, -6.35880162e-04, -5.93661107e-02,\n",
       "        2.52776779e-02, -9.11578536e-03,  3.14962305e-02,  4.85463776e-02,\n",
       "        4.02197335e-03, -4.97474968e-02,  4.02149484e-02, -3.31468172e-02,\n",
       "       -7.33907800e-03, -6.42377287e-02, -7.94854108e-03, -1.31809656e-02,\n",
       "       -1.84598900e-02,  2.87002828e-02,  3.40777934e-02,  1.64637584e-02,\n",
       "        2.55926047e-02,  1.68953408e-02, -7.05589680e-03, -3.55657563e-02,\n",
       "       -1.19185913e-02, -1.83535647e-02, -9.80016496e-03, -3.25880535e-02,\n",
       "       -6.76729009e-02, -4.14830744e-02,  1.26835871e-02, -1.96198467e-02,\n",
       "       -1.33284982e-02,  3.08424380e-04, -2.15604925e-03,  1.49838417e-03,\n",
       "       -3.24675590e-02, -8.34457353e-02, -1.04841879e-02,  3.62600200e-02,\n",
       "        3.27118523e-02, -5.04390635e-02,  1.88516863e-02,  3.44326571e-02,\n",
       "       -5.93706705e-02, -6.96215453e-03, -6.85648760e-03,  3.20587792e-02,\n",
       "        5.67236617e-02,  3.59649328e-03,  3.05235269e-03, -8.35193228e-03,\n",
       "       -6.32247031e-02,  2.40305755e-02,  1.81329083e-02, -3.24354172e-02,\n",
       "        3.55798788e-02,  2.48553678e-02,  2.70361193e-02,  1.49000315e-02,\n",
       "       -1.02008013e-02, -4.06875648e-02,  7.49788946e-03, -7.38299871e-03,\n",
       "       -2.43660919e-02, -2.79312544e-02,  2.15102155e-02, -1.52126793e-03,\n",
       "       -2.60490421e-02, -1.90740209e-02, -5.75376023e-03, -3.91290002e-02,\n",
       "       -3.69491987e-02, -1.56944823e-02,  2.03791447e-03, -2.25926060e-02,\n",
       "        2.28399485e-02,  2.02069841e-02, -1.02493641e-06,  7.31383711e-02,\n",
       "        6.96466342e-02, -2.86335163e-02,  2.49679852e-02,  2.13330723e-02,\n",
       "        1.73934903e-02,  1.66919816e-03,  8.78218487e-02, -4.98331897e-02,\n",
       "       -1.82478987e-02,  6.07356988e-02,  2.91278120e-02, -4.14190348e-03,\n",
       "       -1.23882471e-02, -4.38930988e-02,  3.53288315e-02, -3.17289904e-02,\n",
       "       -1.49161797e-02, -1.92080718e-02, -3.64691652e-02, -4.76264173e-33,\n",
       "       -1.65021289e-02, -1.91781926e-03, -2.67093442e-02, -5.98560683e-02,\n",
       "        2.01788284e-02, -3.64557914e-02,  9.36812758e-02,  4.13829796e-02,\n",
       "        2.49365009e-02,  2.84616631e-02, -5.30352667e-02, -1.58116147e-02,\n",
       "       -1.08963614e-02,  5.59876347e-03,  6.61969604e-03,  4.10367288e-02,\n",
       "        1.94684099e-02,  1.37695195e-02,  2.33071633e-02, -1.79666374e-02,\n",
       "       -9.75467917e-03,  1.38821062e-02,  3.28209549e-02, -7.10959136e-02,\n",
       "        2.42043734e-02,  4.45041843e-02,  6.82343729e-03, -1.49719566e-02,\n",
       "       -1.06187528e-02,  5.69257215e-02, -3.76738347e-02, -2.27492508e-02,\n",
       "        1.36830024e-02,  1.66805815e-02, -2.49875262e-02,  3.49297822e-02,\n",
       "       -2.07296889e-02, -7.13404045e-02, -2.37199496e-02,  1.29399996e-03,\n",
       "       -5.55278957e-02, -4.02691104e-02,  6.04659580e-02, -7.12549686e-02,\n",
       "       -3.67308944e-03,  4.44095284e-02,  3.00362613e-02, -4.66629816e-03,\n",
       "       -3.00806528e-03,  2.76631992e-02, -7.24814162e-02, -1.84943825e-02,\n",
       "       -1.63340047e-02, -7.55406357e-03,  7.54158804e-03, -2.75982190e-02,\n",
       "        1.38857160e-02, -3.57244723e-02, -4.12923768e-02, -2.62139123e-02,\n",
       "       -1.62564665e-02, -5.82111590e-02, -2.01614611e-02,  3.48337251e-03,\n",
       "        3.19005735e-02,  3.50152068e-02,  6.06670119e-02,  3.43761630e-02,\n",
       "       -2.98973490e-02,  3.25451307e-02, -5.99345602e-02,  6.56661987e-02,\n",
       "        2.68801991e-02, -2.69832835e-02, -2.10626107e-02, -1.87442116e-02,\n",
       "       -1.59092993e-02,  1.13463923e-02,  3.98997962e-03,  3.00049345e-04,\n",
       "       -3.66470143e-02, -2.44353935e-02,  6.46924658e-04,  1.99621730e-02,\n",
       "        1.52563527e-02,  5.01294993e-02,  2.02886313e-02,  8.82895663e-03,\n",
       "       -1.11352233e-02,  3.85979936e-02, -1.65022928e-02,  2.19597667e-02,\n",
       "        1.82960406e-02,  3.89181934e-02,  2.53674742e-02, -4.02410254e-02,\n",
       "       -8.96726735e-03, -3.70789021e-02,  2.09788978e-03, -2.77803168e-02,\n",
       "        2.53884345e-02, -5.52188978e-02,  2.19426937e-02, -2.33167014e-03,\n",
       "        5.01943426e-03, -1.64534878e-02,  1.05187446e-02, -5.03660254e-02,\n",
       "       -4.22653966e-02,  7.00085051e-03,  3.10427248e-02, -3.34108323e-02,\n",
       "        5.86859025e-02,  8.19553286e-02,  6.24132603e-02, -2.91639734e-02,\n",
       "        2.99066789e-02, -2.05860827e-02, -1.52513953e-02, -4.37218929e-03,\n",
       "       -3.49394716e-02,  5.21323353e-04, -8.23073313e-02,  1.25809051e-02,\n",
       "        4.63736914e-02, -1.98730882e-02, -6.80074003e-03, -3.20762955e-02,\n",
       "       -3.11929155e-02,  2.33918685e-03, -7.12392945e-03,  3.22027728e-02,\n",
       "        1.92548100e-07,  1.10375823e-03, -2.26693437e-03,  4.66670282e-03,\n",
       "        3.90408486e-02,  1.15278615e-02, -3.16437669e-02,  1.09581770e-02,\n",
       "       -7.46683497e-03, -3.00781615e-03,  4.42970768e-02, -3.87204960e-02,\n",
       "        2.68560983e-02,  6.71141818e-02,  3.13463099e-02,  8.28522909e-03,\n",
       "       -4.69742157e-02, -2.00917423e-02,  1.49600217e-02, -6.05247729e-03,\n",
       "       -2.51887590e-02, -8.65160376e-02, -3.71236587e-03, -1.21992817e-02,\n",
       "        2.72877514e-02,  1.69983767e-02,  1.75527111e-02,  1.60762630e-02,\n",
       "       -2.09924635e-02,  1.62166879e-02, -3.43666575e-03, -4.03561965e-02,\n",
       "        4.92833108e-02, -4.05391343e-02, -2.86077932e-02,  2.82582231e-02,\n",
       "       -5.85249551e-02,  3.73865780e-03, -5.79858832e-02,  1.07589168e-02,\n",
       "        1.01046718e-03, -4.99976873e-02,  7.17913359e-02, -4.38279584e-02,\n",
       "        4.68930379e-02,  1.04924319e-02,  8.27344060e-02, -5.27055352e-04,\n",
       "        1.18511003e-02,  7.17550069e-02, -9.45710298e-03, -1.25319036e-02,\n",
       "       -2.01185346e-02,  3.64994295e-02,  1.48872426e-02, -1.45487045e-03,\n",
       "       -3.00976704e-03,  1.37841189e-02, -1.91234723e-02, -1.82703498e-03,\n",
       "       -8.64937603e-02, -5.01501933e-02, -9.10468865e-03, -1.70881720e-03,\n",
       "        3.53976563e-02,  2.20071990e-02, -3.84717397e-02, -3.58284526e-02,\n",
       "        1.10739075e-34,  7.16629112e-03, -2.17415337e-02, -3.08317668e-03,\n",
       "        6.33605868e-02,  7.69226486e-03, -1.05928788e-02,  6.00480624e-02,\n",
       "       -1.55672869e-02,  5.26661463e-02,  3.67812300e-03, -1.82436276e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = embedding_model.encode(\"My favourite animal is the cow!\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Creating of Embeddings using CPU: It Take More Time That GPU So We will work with GPU:\n",
    "\n",
    "# %%time\n",
    "# embedding_model.to(\"cpu\")\n",
    "\n",
    "# # Embed each chunk one by one\n",
    "# for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "#     item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275589cda6084d38903fc5c5daf4cf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embedding Creation with GPU\n",
    "# %%time\n",
    "embedding_model.to(\"cuda\")\n",
    "\n",
    "# Embed each chunk one by one\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract chunks: 0.0 seconds\n",
      "often. • Calm your “sweet tooth” by eating fruits, such as berries or an apple. • Replace sugary soft drinks with seltzer water, tea, or a small amount of 100 percent fruit juice added to water or soda water. The Food Industry: Functional Attributes of Carbohydrates and the Use of Sugar Substitutes In the food industry, both fast-releasing and slow-releasing carbohydrates are utilized to give foods a wide spectrum of functional attributes, including increased sweetness, viscosity, bulk, coating ability, solubility, consistency, texture, body, and browning capacity. The differences in chemical structure between the different carbohydrates confer their varied functional uses in foods. Starches, gums, and pectins are used as thickening agents in making jam, cakes, cookies, noodles, canned products, imitation cheeses, and a variety of other foods. Molecular gastronomists use slow- releasing carbohydrates, such as alginate, to give shape and texture to their fascinating food creations. Adding fiber to foods increases bulk. Simple sugars are used not only for adding sweetness, but also to add texture, consistency, and browning. In ice cream, the combination of sucrose and corn syrup imparts sweetness as well as a glossy appearance and smooth texture.\n"
     ]
    }
   ],
   "source": [
    "# Embed the Chunk in Batch: This takes less time to create embeddings in batch instead of one by one.\n",
    "\n",
    "# %%time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract the text chunks from the data\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "print(f\"Time taken to extract chunks: {time.time() - start_time} seconds\")\n",
    "print(text_chunks[419])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23.2 s\n",
      "Wall time: 1min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0674,  0.0902, -0.0051,  ..., -0.0221, -0.0232,  0.0126],\n",
       "        [ 0.0552,  0.0592, -0.0166,  ..., -0.0120, -0.0103,  0.0227],\n",
       "        [ 0.0280,  0.0340, -0.0206,  ..., -0.0054,  0.0213,  0.0313],\n",
       "        ...,\n",
       "        [ 0.0771,  0.0098, -0.0122,  ..., -0.0409, -0.0752, -0.0241],\n",
       "        [ 0.1030, -0.0165,  0.0083,  ..., -0.0574, -0.0283, -0.0295],\n",
       "        [ 0.0864, -0.0125, -0.0113,  ..., -0.0522, -0.0337, -0.0299]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Embed all texts in batches: You can see an improvement in embedding the chunks. (Here, Each Chunk Contains Less Than or Equal to 10 Sentences.)\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32,  # You can experiment to find which batch size leads to the best results.\n",
    "                                               convert_to_tensor=True)\n",
    "text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Embedding to File:\n",
    "- You can also save the embedding in vector database for efficent retreiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_number': 277,\n",
       " 'sentence_chunk': 'often. • Calm your “sweet tooth” by eating fruits, such as berries or an apple. • Replace sugary soft drinks with seltzer water, tea, or a small amount of 100 percent fruit juice added to water or soda water. The Food Industry: Functional Attributes of Carbohydrates and the Use of Sugar Substitutes In the food industry, both fast-releasing and slow-releasing carbohydrates are utilized to give foods a wide spectrum of functional attributes, including increased sweetness, viscosity, bulk, coating ability, solubility, consistency, texture, body, and browning capacity. The differences in chemical structure between the different carbohydrates confer their varied functional uses in foods. Starches, gums, and pectins are used as thickening agents in making jam, cakes, cookies, noodles, canned products, imitation cheeses, and a variety of other foods. Molecular gastronomists use slow- releasing carbohydrates, such as alginate, to give shape and texture to their fascinating food creations. Adding fiber to foods increases bulk. Simple sugars are used not only for adding sweetness, but also to add texture, consistency, and browning. In ice cream, the combination of sucrose and corn syrup imparts sweetness as well as a glossy appearance and smooth texture.',\n",
       " 'chunk_char_count': 1264,\n",
       " 'chunk_word_count': 190,\n",
       " 'chunk_token_count': 316.0,\n",
       " 'embedding': array([ 3.36195491e-02, -6.33054134e-03,  1.35672670e-02, -1.72721618e-03,\n",
       "         7.37232668e-03, -1.12093491e-02, -7.77235404e-02, -1.98784610e-03,\n",
       "         4.41749059e-02,  1.84355155e-02,  1.08431850e-03, -4.48691770e-02,\n",
       "        -1.44435819e-02,  2.23461557e-02, -1.58056766e-02, -3.96063253e-02,\n",
       "         6.19645789e-03,  3.06795072e-02,  1.50323585e-02, -3.01858541e-02,\n",
       "        -5.63731277e-03,  7.23950379e-03,  4.44136746e-03,  3.83775122e-02,\n",
       "        -2.77931411e-02, -1.08045274e-02, -2.09066980e-02, -3.82517441e-03,\n",
       "         1.28809176e-02, -7.62624517e-02,  2.56193765e-02,  5.03177457e-02,\n",
       "        -1.48145333e-02, -9.30032358e-02,  1.84821761e-06, -2.72034127e-02,\n",
       "         2.04363372e-02,  3.49979401e-02, -7.01642632e-02,  8.21791142e-02,\n",
       "         9.58013162e-03,  1.12016946e-02,  1.45271542e-02,  2.08604969e-02,\n",
       "         1.88052398e-03,  1.94313359e-02,  4.30906154e-02, -6.63991868e-02,\n",
       "        -4.47116373e-03,  9.62139573e-03, -1.76184997e-02, -9.32419896e-02,\n",
       "        -1.74521934e-02,  1.43797649e-02,  9.59808528e-02,  1.59285665e-02,\n",
       "         3.41023952e-02,  9.89576429e-03,  2.74769054e-03, -3.97160351e-02,\n",
       "         1.76487546e-02, -2.50339173e-02,  4.65405360e-02,  5.50866732e-03,\n",
       "         3.66739631e-02,  8.11415091e-02,  1.02665520e-03, -3.91496392e-03,\n",
       "         1.02581047e-02,  3.18636261e-02, -4.20973934e-02, -6.66076457e-03,\n",
       "         2.48229317e-02,  1.19641889e-02, -2.52436381e-02,  1.71536338e-02,\n",
       "         3.19568291e-02, -4.50095460e-02,  2.03784946e-02,  3.80982012e-02,\n",
       "        -2.11241934e-02,  1.18820909e-02, -3.76307609e-04, -8.87259841e-04,\n",
       "         7.49149919e-02, -1.01155806e-02, -2.94809155e-02,  7.10156607e-03,\n",
       "         6.48592636e-02, -1.94349668e-05, -1.11182481e-01, -5.66404425e-02,\n",
       "        -2.52492689e-02,  1.38240708e-02,  7.55137904e-03, -1.46898367e-02,\n",
       "         2.22207345e-02,  5.33666753e-04,  2.86545530e-02, -6.54062629e-02,\n",
       "         2.24799272e-02,  5.24192117e-02, -2.71777790e-02,  8.84033088e-03,\n",
       "        -1.05444733e-02, -4.95076703e-04, -1.30082797e-02,  3.40696275e-02,\n",
       "         2.01095572e-05, -1.76007822e-02,  8.07830784e-03, -6.34983601e-03,\n",
       "        -4.88275215e-02,  4.00034562e-02,  1.75283067e-02,  9.74304229e-03,\n",
       "         1.14658172e-03,  6.90652104e-03, -3.30178477e-02, -3.02616414e-02,\n",
       "        -3.29962485e-02,  2.22184397e-02,  3.96934114e-02,  3.49612869e-02,\n",
       "        -8.87161344e-02,  1.60383154e-02, -1.26950315e-03, -2.32299678e-02,\n",
       "        -1.26420315e-02,  2.95909680e-02,  2.31829137e-02,  1.65003259e-02,\n",
       "        -3.81323397e-02,  3.05584334e-02,  4.51847278e-02,  2.07054559e-02,\n",
       "        -1.63254805e-03,  6.37390791e-03, -2.33271811e-02, -5.73715195e-02,\n",
       "        -5.11532314e-02, -4.93464321e-02, -3.44207548e-02, -2.04843991e-02,\n",
       "        -2.87391711e-02,  5.63038476e-02, -8.33713450e-03,  2.81263096e-03,\n",
       "         2.13627163e-02, -1.49661936e-02, -5.19253463e-02,  2.39164326e-02,\n",
       "         4.13530953e-02,  3.56319211e-02,  8.95308703e-03, -3.69550288e-02,\n",
       "        -3.16564105e-02, -9.72755719e-03, -2.04478316e-02,  8.17026049e-02,\n",
       "        -1.99236497e-02, -1.28493626e-02,  1.64393079e-03, -4.63711172e-02,\n",
       "        -1.17448121e-02,  4.12844568e-02, -5.67119103e-03,  2.24809851e-02,\n",
       "        -3.83236781e-02, -1.59223340e-02,  2.21639443e-02, -2.22279895e-02,\n",
       "        -1.54940588e-02, -1.01862766e-03,  7.21554756e-02, -2.41180919e-02,\n",
       "        -1.79879460e-02,  9.44163371e-03, -3.65188122e-02, -1.49371114e-03,\n",
       "         6.17680401e-02,  5.66991530e-02,  4.52418663e-02,  2.77483221e-02,\n",
       "        -3.50607671e-02,  9.84825380e-03,  3.15149762e-02,  5.07388562e-02,\n",
       "        -1.77961532e-02, -3.29665616e-02,  1.27585661e-02,  2.99521983e-02,\n",
       "         3.04353014e-02,  1.07565038e-02,  8.39108694e-03,  3.33771072e-02,\n",
       "        -4.95182313e-02,  2.59655975e-02,  2.12876732e-03, -2.50232435e-04,\n",
       "         6.51650280e-02,  5.81840873e-02,  3.07668019e-02, -2.88984901e-03,\n",
       "        -1.51140923e-02, -3.88626894e-03, -2.01706178e-02,  4.65609320e-02,\n",
       "        -1.68076735e-02, -6.59578070e-02, -1.26033947e-02, -1.55909834e-02,\n",
       "        -6.79307210e-04, -3.24724913e-02, -6.40973169e-03,  3.73333134e-03,\n",
       "        -1.18836546e-02,  3.81205380e-02, -2.22536605e-02,  5.33905812e-02,\n",
       "         3.00371051e-02,  1.18945753e-02,  5.80814742e-02, -2.68650074e-02,\n",
       "         7.29336888e-02,  2.93379538e-02,  5.82008809e-02, -5.18405326e-02,\n",
       "        -2.97832713e-02,  3.41012441e-02,  3.58817354e-02,  2.01626793e-02,\n",
       "         1.53031026e-03,  3.30132106e-03, -2.03335788e-02,  2.88390424e-02,\n",
       "         1.79472715e-02,  3.13740335e-02,  5.43468166e-03,  5.02343960e-02,\n",
       "        -2.97928322e-02, -5.50696366e-02, -6.76095532e-03, -7.79474387e-04,\n",
       "        -1.20694395e-02,  2.82925013e-02,  2.94356258e-03, -2.83945166e-02,\n",
       "        -2.30177734e-02, -9.66009218e-03, -4.08907095e-03, -3.00462134e-02,\n",
       "        -5.52085647e-03,  3.04490933e-03, -1.92805678e-02, -1.19101554e-02,\n",
       "         1.14658219e-03, -2.52537616e-02,  2.08762269e-02,  7.25417398e-03,\n",
       "         6.35353178e-02, -2.79531116e-03,  1.50288194e-02,  3.25876996e-02,\n",
       "        -6.62059113e-02, -6.57664835e-02, -4.48119678e-02,  5.92018180e-02,\n",
       "         4.68918402e-03, -1.12712122e-02, -1.91877838e-02,  5.61325587e-02,\n",
       "         4.85782661e-02,  2.94703022e-02,  3.55923995e-02,  5.80264349e-03,\n",
       "         4.88801254e-03, -4.76187915e-02, -3.37588564e-02, -8.45589302e-03,\n",
       "         1.14556830e-02,  2.67739259e-02,  1.23477122e-02,  2.00255346e-02,\n",
       "         1.81062091e-02,  6.46771351e-03, -6.97586834e-02, -2.74993442e-02,\n",
       "        -7.13094044e-03, -2.73372792e-02,  7.44503811e-02,  1.47062810e-02,\n",
       "        -1.30421212e-02,  9.73013509e-03, -3.36028300e-02,  2.02024356e-02,\n",
       "         3.13419551e-02, -7.58524798e-03, -1.71297893e-03, -2.14764886e-02,\n",
       "         1.32148322e-02,  2.89805862e-03,  5.57679590e-03,  1.89360157e-02,\n",
       "        -7.54978461e-03,  5.85188530e-02, -3.90176149e-03,  9.89175984e-04,\n",
       "         1.98942516e-02,  8.60823784e-03,  2.15447638e-02,  2.29394678e-02,\n",
       "         6.50781579e-03,  4.21973281e-02,  8.26636236e-03, -2.81754471e-02,\n",
       "         7.21110730e-03, -2.69675311e-02,  7.82311857e-02, -2.18505748e-02,\n",
       "        -1.21231917e-02,  1.76022388e-02, -1.50717124e-02, -1.42516978e-02,\n",
       "        -8.10357928e-03,  3.10534728e-03, -1.54149195e-04, -1.05307931e-02,\n",
       "        -8.38614330e-02,  2.03410182e-02, -2.97046807e-02,  2.18859129e-02,\n",
       "         3.91102210e-02,  4.04879749e-02,  1.79388262e-02, -7.44180242e-03,\n",
       "        -1.97079033e-02,  9.81899500e-02, -3.58328857e-02, -5.74958175e-02,\n",
       "        -1.95771325e-02, -4.07495908e-02, -2.19417810e-02, -2.51251981e-02,\n",
       "         5.03383651e-02, -9.53014940e-03,  7.42822587e-02, -3.41526960e-04,\n",
       "        -3.74953845e-03,  2.93305069e-02,  1.55238400e-03, -2.88340542e-02,\n",
       "        -1.85488611e-02, -1.90557688e-02, -4.09590118e-02, -3.18212286e-02,\n",
       "         2.95139644e-02,  3.23085277e-03, -2.50307973e-02,  1.53751057e-02,\n",
       "         4.33425382e-02,  5.10641746e-03, -2.58109290e-02, -7.54621550e-02,\n",
       "        -1.04839103e-02,  4.82506417e-02, -1.21016549e-02, -5.31354770e-02,\n",
       "         2.99499556e-02, -3.44398096e-02,  1.44982226e-02,  3.35098058e-02,\n",
       "         4.15852219e-02, -1.07931793e-02,  1.73437074e-02,  1.92255247e-02,\n",
       "         2.45624334e-02,  4.63915505e-02, -5.21127833e-03, -1.41820638e-02,\n",
       "         4.22138125e-02, -6.33322224e-02,  4.98141870e-02, -2.07672752e-02,\n",
       "        -7.48898089e-02, -2.81311553e-02, -7.68958731e-03, -3.19059901e-02,\n",
       "        -1.94262322e-02, -3.94592322e-02,  3.08891553e-02,  5.22624888e-03,\n",
       "        -2.11188607e-02,  2.72949189e-02,  3.95513140e-02, -2.35161260e-02,\n",
       "         2.23498866e-02,  1.15609569e-02, -4.94572222e-02,  3.27671692e-02,\n",
       "         6.16607890e-02, -4.80517410e-02,  1.66053399e-02, -3.90644334e-02,\n",
       "         1.80994105e-02, -4.69239652e-02,  2.08608154e-02, -4.26506530e-03,\n",
       "         2.95830742e-02,  2.67971735e-02,  2.06797360e-03,  1.40325855e-02,\n",
       "         1.39026623e-02, -3.42218392e-02,  4.16489914e-02,  4.45410423e-02,\n",
       "         2.13968661e-02,  2.67350096e-02,  2.34730970e-02,  4.50120158e-02,\n",
       "        -6.25007749e-02, -8.93838424e-03, -1.02569722e-02,  3.47212926e-02,\n",
       "        -2.72306800e-02, -5.27836867e-02, -6.72355220e-02, -2.41084695e-02,\n",
       "        -6.29305989e-02, -2.45558172e-02,  6.79910332e-02, -1.71199273e-02,\n",
       "        -5.19692106e-03, -2.50085033e-02,  4.36817907e-04, -5.29314391e-04,\n",
       "         5.64266704e-02, -3.68509814e-02,  1.27865095e-03,  4.36808914e-02,\n",
       "         3.42692994e-03,  7.88339972e-03, -4.34135273e-02, -4.49230447e-02,\n",
       "         8.90783519e-02,  1.74692720e-02,  2.03296468e-02, -1.01353684e-02,\n",
       "        -9.39848274e-03,  8.80940408e-02, -5.66901732e-03, -1.05300779e-02,\n",
       "        -6.30244315e-02,  6.76475791e-03,  1.29811699e-03, -3.32524590e-02,\n",
       "        -6.34877905e-02,  7.06905946e-02, -5.23131201e-03, -2.53332257e-02,\n",
       "        -2.87126545e-02, -1.01521621e-02,  4.22700085e-02, -1.54305287e-02,\n",
       "        -7.59011507e-02,  3.65060419e-02,  4.00408134e-02,  2.08073054e-02,\n",
       "        -9.66927782e-03,  7.80478818e-03, -2.30108034e-02,  6.49472550e-02,\n",
       "        -2.14785691e-02,  1.98700605e-03, -1.97605998e-03, -4.29251567e-02,\n",
       "        -6.99224323e-02,  5.22380546e-02, -7.91266486e-02,  2.38740984e-02,\n",
       "        -4.35846038e-02,  1.17662121e-02,  1.32625662e-02,  6.97017163e-02,\n",
       "         4.33111899e-02, -4.57390165e-03,  6.64432123e-02, -3.31922993e-02,\n",
       "         1.59749966e-02, -5.51458523e-02, -1.20564699e-02,  5.07311383e-03,\n",
       "        -2.47540604e-02, -6.06123209e-02, -5.29981032e-03, -4.35959771e-02,\n",
       "         4.77819480e-02,  3.37773040e-02, -2.80110296e-02,  6.84495568e-02,\n",
       "         6.60216063e-02,  2.77554896e-03,  3.83193195e-02, -4.94900532e-02,\n",
       "         1.73597615e-02, -2.41690017e-02, -2.11087205e-02,  4.73574828e-03,\n",
       "         3.32153402e-02,  5.93856126e-02,  9.94154252e-03,  2.92045977e-02,\n",
       "        -2.83260010e-02,  3.52802873e-02, -4.07908969e-02, -5.74266501e-02,\n",
       "         2.63905078e-02, -7.08783865e-02,  1.73162278e-02,  5.38092293e-03,\n",
       "         1.15250861e-02, -2.14818623e-02,  2.60816468e-03,  3.82594317e-02,\n",
       "        -6.18264861e-02,  9.82395187e-03,  3.29365842e-02,  9.30186827e-03,\n",
       "        -6.35777637e-02,  8.44492763e-02,  1.37456425e-03,  1.92112487e-03,\n",
       "        -2.84220222e-02,  4.92005832e-02,  1.61023680e-02,  1.69644132e-02,\n",
       "         1.62301566e-02,  2.92289327e-03,  3.85009237e-02, -4.31264564e-02,\n",
       "         1.44625651e-02, -1.80732496e-02,  5.74169382e-02, -3.90736982e-02,\n",
       "         8.29734281e-02, -1.12668332e-02, -5.81889264e-02, -3.74181308e-02,\n",
       "         1.12795550e-02, -1.26190066e-01,  5.36711887e-03, -4.74464847e-03,\n",
       "        -3.80182602e-02, -3.80318090e-02,  3.38512547e-02, -5.65267389e-33,\n",
       "         3.12505849e-02, -2.74420790e-02,  1.49309905e-02,  9.66008753e-02,\n",
       "         3.82222943e-02,  6.07445017e-02, -8.67401436e-03, -2.20907908e-02,\n",
       "        -3.87738384e-02,  3.14043872e-02, -2.78094131e-02, -2.61556134e-02,\n",
       "        -2.18033418e-03, -5.67599460e-02, -3.29962522e-02, -3.37906703e-02,\n",
       "         2.51040794e-02,  1.20999031e-02, -4.59938589e-03, -2.30383850e-03,\n",
       "        -8.03607926e-02, -2.47460846e-02,  3.90837193e-02, -3.80970016e-02,\n",
       "         7.66094476e-02,  3.75051200e-02,  1.00947553e-02, -7.00766295e-02,\n",
       "        -2.27734931e-02,  3.35722789e-02, -2.90578455e-02, -3.86458915e-03,\n",
       "         3.98564227e-02, -3.37578394e-02, -2.65688039e-02,  8.76989365e-02,\n",
       "         8.22847709e-03, -1.56535283e-02, -1.40035590e-02,  2.47986913e-02,\n",
       "        -3.26930992e-02, -2.71499865e-02, -2.48089060e-02, -1.73133574e-02,\n",
       "        -4.77490686e-02,  4.30413298e-02,  1.23732947e-02,  1.67823285e-02,\n",
       "         5.97122423e-02,  1.10688535e-02, -2.47990526e-02, -4.01357096e-03,\n",
       "         3.85955013e-02,  7.99255725e-03,  4.08465900e-02,  8.16078857e-02,\n",
       "         1.41681163e-02, -1.61755346e-02,  4.62705903e-02,  6.75018355e-02,\n",
       "         4.88050375e-03,  4.43557426e-02, -5.26424646e-02,  4.46237586e-02,\n",
       "         1.51577527e-02, -3.11318282e-02, -2.43333867e-03, -1.35898320e-02,\n",
       "        -5.16744927e-02, -1.28283305e-02, -4.79294285e-02,  1.57604318e-05,\n",
       "         5.55568039e-02,  7.30251102e-03,  1.25851378e-01, -7.46830851e-02,\n",
       "        -3.31933759e-02, -2.04487164e-02, -8.40956643e-02, -9.70691368e-02,\n",
       "        -2.75872764e-04, -2.53044069e-02,  3.13058123e-02, -2.38177367e-02,\n",
       "        -2.50919331e-02,  1.06879972e-01, -8.03863723e-03, -4.56796661e-02,\n",
       "        -1.87865123e-02, -6.08093925e-02,  4.03947979e-02,  6.67785620e-03,\n",
       "        -3.44066173e-02,  5.60434684e-02,  2.06243098e-02,  7.52734253e-03,\n",
       "         3.48817185e-02, -8.49592034e-03,  2.34857295e-03, -5.82642527e-03,\n",
       "        -4.84207682e-02,  2.79288813e-02,  2.19938010e-02,  1.34437289e-02,\n",
       "        -3.05742491e-02, -2.82639321e-02, -5.49180023e-02,  4.83709499e-02,\n",
       "         2.10338403e-02, -4.58570383e-02, -8.81959498e-03,  3.36518995e-02,\n",
       "         3.62561308e-02, -5.10729030e-02, -5.84396124e-02,  7.98110967e-04,\n",
       "         1.99138187e-02,  2.59554517e-02, -9.53386165e-03, -2.35024560e-02,\n",
       "        -6.40766099e-02, -9.99842305e-03,  6.02089390e-02, -1.54787861e-02,\n",
       "         1.46333240e-02, -2.73179263e-02,  2.11187713e-02, -7.98223317e-02,\n",
       "        -6.87027127e-02,  9.48770065e-03, -1.09383371e-02,  1.36643257e-02,\n",
       "         2.81300316e-07,  1.70468353e-02,  1.37893427e-02, -1.67372134e-02,\n",
       "         3.99160236e-02, -5.93482330e-02, -8.79669189e-03,  4.47691604e-02,\n",
       "        -2.94926446e-02, -3.48558277e-02, -3.94704826e-02,  1.52493094e-03,\n",
       "        -5.70393354e-02, -1.58384964e-02,  2.68248525e-02, -2.32512355e-02,\n",
       "         1.21564167e-02, -1.29620079e-02, -6.76470995e-02,  3.04765757e-02,\n",
       "         2.22088955e-02, -7.54230190e-03, -1.48998573e-02, -1.43286055e-02,\n",
       "        -3.65560204e-02, -3.22323516e-02,  1.44278845e-02, -5.36744529e-03,\n",
       "        -1.84828974e-02,  6.61565512e-02,  2.09395252e-02,  3.05021810e-03,\n",
       "         1.59577411e-02,  3.94967459e-02, -2.02135909e-02, -2.40854360e-02,\n",
       "        -2.85920072e-02, -2.59215720e-02, -8.99069682e-02, -1.97141394e-02,\n",
       "        -8.99936780e-02, -3.42065541e-05,  1.10064936e-03, -2.26069931e-02,\n",
       "         1.26895611e-03, -3.18909474e-02, -3.87221277e-02, -7.64141697e-03,\n",
       "         1.19272703e-02,  6.19138628e-02, -5.65737449e-02, -9.06444434e-03,\n",
       "         4.63988148e-02, -2.26716660e-02,  5.40676177e-04,  3.30266682e-03,\n",
       "         7.85038981e-04, -5.53774927e-03,  2.09835358e-02,  1.68783143e-02,\n",
       "         7.32700573e-03,  1.08059188e-02,  3.89462113e-02, -3.56237181e-02,\n",
       "         1.80466659e-02, -1.45939384e-02,  4.73402999e-03,  1.92299914e-02,\n",
       "         2.68856405e-34,  1.68734277e-03, -5.82957901e-02,  2.25708280e-02,\n",
       "         6.02852087e-03, -3.00494377e-02,  3.22193913e-02, -1.27366362e-02,\n",
       "         3.73876691e-02,  2.93491012e-03,  4.22898829e-02, -1.54303224e-03],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_min_token_len[419]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "      <td>[ 6.74242675e-02  9.02281255e-02 -5.09549351e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "      <td>[ 5.52156232e-02  5.92139661e-02 -1.66167356e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
       "      <td>766</td>\n",
       "      <td>114</td>\n",
       "      <td>191.50</td>\n",
       "      <td>[ 2.79801954e-02  3.39813679e-02 -2.06426606e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
       "      <td>941</td>\n",
       "      <td>142</td>\n",
       "      <td>235.25</td>\n",
       "      <td>[ 6.82566836e-02  3.81274670e-02 -8.46854784e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35</td>\n",
       "      <td>The Cardiovascular System University of Hawai‘...</td>\n",
       "      <td>998</td>\n",
       "      <td>152</td>\n",
       "      <td>249.50</td>\n",
       "      <td>[ 3.30264494e-02 -8.49767216e-03  9.57160257e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "1          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "2          -37  Contents Preface University of Hawai‘i at Māno...   \n",
       "3          -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
       "4          -35  The Cardiovascular System University of Hawai‘...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               308                42              77.00   \n",
       "1               210                30              52.50   \n",
       "2               766               114             191.50   \n",
       "3               941               142             235.25   \n",
       "4               998               152             249.50   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 6.74242675e-02  9.02281255e-02 -5.09549351e-...  \n",
       "1  [ 5.52156232e-02  5.92139661e-02 -1.66167356e-...  \n",
       "2  [ 2.79801954e-02  3.39813679e-02 -2.06426606e-...  \n",
       "3  [ 6.82566836e-02  3.81274670e-02 -8.46854784e-...  \n",
       "4  [ 3.30264494e-02 -8.49767216e-03  9.57160257e-...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import saved file and view \n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG: Search & Answer: (Semantic Search)\n",
    "\n",
    "- RAG goal: Retrieve relevant passages based on a query and use those passages to augment an input to an LLM so it can generate an output based on those relevant passages.\n",
    "\n",
    "- Similarity Search: To Find Similar Passage and Context based on Query. Comparing embeddings is known as similarity search, vector search, semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "      <td>[0.0674242675, 0.0902281255, -0.00509549351, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "      <td>[0.0552156232, 0.0592139661, -0.0166167356, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
       "      <td>766</td>\n",
       "      <td>114</td>\n",
       "      <td>191.50</td>\n",
       "      <td>[0.0279801954, 0.0339813679, -0.0206426606, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
       "      <td>941</td>\n",
       "      <td>142</td>\n",
       "      <td>235.25</td>\n",
       "      <td>[0.0682566836, 0.038127467, -0.00846854784, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35</td>\n",
       "      <td>The Cardiovascular System University of Hawai‘...</td>\n",
       "      <td>998</td>\n",
       "      <td>152</td>\n",
       "      <td>249.50</td>\n",
       "      <td>[0.0330264494, -0.00849767216, 0.00957160257, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>1164</td>\n",
       "      <td>Flashcard Images Note: Most images in the flas...</td>\n",
       "      <td>1305</td>\n",
       "      <td>176</td>\n",
       "      <td>326.25</td>\n",
       "      <td>[0.0185622647, -0.0164278075, -0.0127045643, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>1164</td>\n",
       "      <td>Hazard Analysis Critical Control Points reused...</td>\n",
       "      <td>375</td>\n",
       "      <td>51</td>\n",
       "      <td>93.75</td>\n",
       "      <td>[0.03347205, -0.0570440702, 0.0151489452, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>1165</td>\n",
       "      <td>ShareAlike 11. Organs reused “Pancreas Organ A...</td>\n",
       "      <td>1286</td>\n",
       "      <td>173</td>\n",
       "      <td>321.50</td>\n",
       "      <td>[0.0770515576, 0.00978558045, -0.0121817328, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>1165</td>\n",
       "      <td>Sucrose reused “Figure 03 02 05” by OpenStax B...</td>\n",
       "      <td>410</td>\n",
       "      <td>59</td>\n",
       "      <td>102.50</td>\n",
       "      <td>[0.103045173, -0.0164701752, 0.00826843735, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>1166</td>\n",
       "      <td>23. Vitamin D reused “The Functions of Vitamin...</td>\n",
       "      <td>250</td>\n",
       "      <td>37</td>\n",
       "      <td>62.50</td>\n",
       "      <td>[0.0863773897, -0.0125358999, -0.0112746665, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1680 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_number                                     sentence_chunk  \\\n",
       "0             -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "1             -38  Human Nutrition: 2020 Edition by University of...   \n",
       "2             -37  Contents Preface University of Hawai‘i at Māno...   \n",
       "3             -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
       "4             -35  The Cardiovascular System University of Hawai‘...   \n",
       "...           ...                                                ...   \n",
       "1675         1164  Flashcard Images Note: Most images in the flas...   \n",
       "1676         1164  Hazard Analysis Critical Control Points reused...   \n",
       "1677         1165  ShareAlike 11. Organs reused “Pancreas Organ A...   \n",
       "1678         1165  Sucrose reused “Figure 03 02 05” by OpenStax B...   \n",
       "1679         1166  23. Vitamin D reused “The Functions of Vitamin...   \n",
       "\n",
       "      chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0                  308                42              77.00   \n",
       "1                  210                30              52.50   \n",
       "2                  766               114             191.50   \n",
       "3                  941               142             235.25   \n",
       "4                  998               152             249.50   \n",
       "...                ...               ...                ...   \n",
       "1675              1305               176             326.25   \n",
       "1676               375                51              93.75   \n",
       "1677              1286               173             321.50   \n",
       "1678               410                59             102.50   \n",
       "1679               250                37              62.50   \n",
       "\n",
       "                                              embedding  \n",
       "0     [0.0674242675, 0.0902281255, -0.00509549351, -...  \n",
       "1     [0.0552156232, 0.0592139661, -0.0166167356, -0...  \n",
       "2     [0.0279801954, 0.0339813679, -0.0206426606, 0....  \n",
       "3     [0.0682566836, 0.038127467, -0.00846854784, -0...  \n",
       "4     [0.0330264494, -0.00849767216, 0.00957160257, ...  \n",
       "...                                                 ...  \n",
       "1675  [0.0185622647, -0.0164278075, -0.0127045643, -...  \n",
       "1676  [0.03347205, -0.0570440702, 0.0151489452, -0.0...  \n",
       "1677  [0.0770515576, 0.00978558045, -0.0121817328, 0...  \n",
       "1678  [0.103045173, -0.0164701752, 0.00826843735, 0....  \n",
       "1679  [0.0863773897, -0.0125358999, -0.0112746665, 0...  \n",
       "\n",
       "[1680 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert our embeddings into a torch.tensor\n",
    "embeddings = torch.tensor(np.stack(text_chunks_and_embedding_df[\"embedding\"].tolist(), axis=0), dtype=torch.float32).to(device)\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "text_chunks_and_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1680, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",   # Loaded from Cache. Because we downloaded previously.\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Search Pipeline:\n",
    "Let's create a small semantic search pipeline. In essence, we want to search for a query (e.g. \"macronutrient functions\") and get back relevant passages from our textbook.\n",
    "\n",
    "We can do so with the following steps:\n",
    "1. Define a query string.\n",
    "2. Turn the query string into an embedding.\n",
    "3. Perform a dot product or cosine similarity function between the text embeddings and the query embedding.\n",
    "4. Sort the results from 3 in descending order.\n",
    "\n",
    "Note: to use dot product for comparison, ensure vector sizes are of same shape (e.g. 768) and tensors/vectors are in the same datatype (e.g. both are in torch.float32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: good foods for protein\n",
      "[INFO] Time taken to get scores on 1680 embeddings: 0.00132 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.7729, 0.7647, 0.6743, 0.6743, 0.6634], device='cuda:0'),\n",
       "indices=tensor([611, 616, 615, 620, 617], device='cuda:0'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the query\n",
    "query = \"good foods for protein\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query\n",
    "# Note: it's import to embed you query with the same model you embedding your passages\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(\"cuda\")\n",
    "\n",
    "# 3. Get similarity scores with the dot product (use cosine similarity if outputs of model aren't normalized)\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer() \n",
    "\n",
    "print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep top 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)  # Torch Method to find top 5 results from the dot product result.\n",
    "top_results_dot_product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dietary Sources of Protein The protein food group consists of foods made from meat, seafood, poultry, eggs, soy, dry beans, peas, and seeds. According to the Harvard School of Public Health, “animal protein and vegetable protein probably have the same effects on health. It’s the protein package that’s likely to make a difference.”1 1. Protein: The Bottom Line. Harvard School of Public Proteins, Diet, and Personal Choices | 411\n",
      "Additionally, a person should consume 8 ounces of cooked seafood every week (typically as two 4-ounce servings) to assure they are getting the healthy omega-3 fatty acids that have been linked to a lower risk for heart disease. Another tip is choosing to eat dry beans, peas, or soy products as a main dish. Some of the menu choices include chili with kidney and pinto beans, hummus on pita bread, and black bean enchiladas. You could also enjoy nuts in a variety of ways. You can put them on a salad, in a stir-fry, or use them as a topping for steamed vegetables in place of meat or cheese. If you do not eat meat, the USDA has much more information on how to get all the protein you need from a plant-based diet. When choosing the best protein-rich foods to eat, pay attention to the whole nutrient package and remember to select from a variety of protein sources to get all the other essential micronutrients. Protein Quality While protein is contained in a wide variety of foods, it differs in quality. High-quality protein contains all the essential amino acids in the proportions needed by the human body. The amino acid profile of different foods is therefore one component of protein quality.\n",
      "Food PDCAAS* Milk protein 1.00 Egg white 1.00 Whey 1.00 Soy protein 1.00 Beef 0.92 Soybeans 0.91 Chickpeas 0.78 Fruits 0.76 Vegetables 0.73 Whole wheat 0.42 *1 is the highest rank, 0 is the lowest Protein Needs: Special Considerations Some groups may need to examine how to meet their protein needs more closely than others. We will take a closer look at the special protein considerations for vegetarians, the elderly, and athletes. Vegetarians and Vegans People who follow variations of the vegetarian diet and consume eggs and/or dairy products can meet their protein requirements by consuming adequate amounts of these foods. Vegetarians and vegans can also attain their recommended protein intakes if they give a little more attention to high-quality plant-based protein sources. However, when following a vegetarian diet, the amino acid lysine can be challenging to acquire. Grains, nuts, and seeds are lysine-poor foods, but tofu, soy, quinoa, and pistachios are all good Proteins, Diet, and Personal Choices | 417\n"
     ]
    }
   ],
   "source": [
    "# To Match the Extracted Indices Number with The Query:\n",
    "print(pages_and_chunks[611]['sentence_chunk'])\n",
    "print(pages_and_chunks[616]['sentence_chunk'])\n",
    "print(pages_and_chunks[620]['sentence_chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000*1680*150 # Embeddings*Average Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([168000, 768])\n",
      "[INFO] Time taken to get scores on 168000 embeddings: 0.00068 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Example to Test How Work fast Semantic Search:\n",
    "larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n",
    "print(f\"Embeddings shape: {larger_embeddings.shape}\")\n",
    "\n",
    "# Perform dot product across 168,000 embeddings\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
    "end_time = timer() \n",
    "\n",
    "print(f\"[INFO] Time taken to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that searching over embeddings is very fast even if we do exhaustive.\n",
    "\n",
    "But if you had 10M+ embeddings, you likely want to create an index.\n",
    "\n",
    "An index is like letters in the dictionary.\n",
    "\n",
    "For example, if you wanted to search \"duck\" in the dictionary, you'd start at \"d\" then find words close to \"du...\" etc.\n",
    "\n",
    "An index helps to narrow it down.\n",
    "\n",
    "A populary indexing library for vector search is Faiss, see here: https://github.com/facebookresearch/faiss \n",
    "\n",
    "One technique that the library provides is approximate nearest neighbour search (ANN): https://en.wikipedia.org/wiki/Nearest_neighbor_search\n",
    "\n",
    "Let's make our vector search results pretty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'good foods for protein'\n",
      "\n",
      "Results:\n",
      "Score: 0.7729\n",
      "Text:\n",
      "Dietary Sources of Protein The protein food group consists of foods made from\n",
      "meat, seafood, poultry, eggs, soy, dry beans, peas, and seeds. According to the\n",
      "Harvard School of Public Health, “animal protein and vegetable protein probably\n",
      "have the same effects on health. It’s the protein package that’s likely to make\n",
      "a difference.”1 1. Protein: The Bottom Line. Harvard School of Public Proteins,\n",
      "Diet, and Personal Choices | 411\n",
      "Page number: 411\n",
      "\n",
      "\n",
      "Score: 0.7647\n",
      "Text:\n",
      "Additionally, a person should consume 8 ounces of cooked seafood every week\n",
      "(typically as two 4-ounce servings) to assure they are getting the healthy\n",
      "omega-3 fatty acids that have been linked to a lower risk for heart disease.\n",
      "Another tip is choosing to eat dry beans, peas, or soy products as a main dish.\n",
      "Some of the menu choices include chili with kidney and pinto beans, hummus on\n",
      "pita bread, and black bean enchiladas. You could also enjoy nuts in a variety of\n",
      "ways. You can put them on a salad, in a stir-fry, or use them as a topping for\n",
      "steamed vegetables in place of meat or cheese. If you do not eat meat, the USDA\n",
      "has much more information on how to get all the protein you need from a plant-\n",
      "based diet. When choosing the best protein-rich foods to eat, pay attention to\n",
      "the whole nutrient package and remember to select from a variety of protein\n",
      "sources to get all the other essential micronutrients. Protein Quality While\n",
      "protein is contained in a wide variety of foods, it differs in quality. High-\n",
      "quality protein contains all the essential amino acids in the proportions needed\n",
      "by the human body. The amino acid profile of different foods is therefore one\n",
      "component of protein quality.\n",
      "Page number: 414\n",
      "\n",
      "\n",
      "Score: 0.6743\n",
      "Text:\n",
      "18.9 5.4 200 454 Tuna 3 oz. (canned) 21.7 0.2 26 99 Soybeans 1 c. (boiled) 29.0\n",
      "2.2 0 298 Lentils 1 c. (boiled) 17.9 0.1 0 226 Kidney beans 1 c. (canned) 13.5\n",
      "0.2 0 215 Sunflower seeds 1 c. 9.6 2.0 0 269 The USDA provides some tips for\n",
      "choosing your dietary protein sources. Their motto is, “Go Lean with Protein”.\n",
      "The overall suggestion is to eat a variety of protein-rich foods to benefit\n",
      "health. The USDA recommends lean meats, such as round steaks, top sirloin, extra\n",
      "lean ground beef, pork loin, and skinless chicken. Proteins, Diet, and Personal\n",
      "Choices | 413\n",
      "Page number: 413\n",
      "\n",
      "\n",
      "Score: 0.6743\n",
      "Text:\n",
      "Food PDCAAS* Milk protein 1.00 Egg white 1.00 Whey 1.00 Soy protein 1.00 Beef\n",
      "0.92 Soybeans 0.91 Chickpeas 0.78 Fruits 0.76 Vegetables 0.73 Whole wheat 0.42\n",
      "*1 is the highest rank, 0 is the lowest Protein Needs: Special Considerations\n",
      "Some groups may need to examine how to meet their protein needs more closely\n",
      "than others. We will take a closer look at the special protein considerations\n",
      "for vegetarians, the elderly, and athletes. Vegetarians and Vegans People who\n",
      "follow variations of the vegetarian diet and consume eggs and/or dairy products\n",
      "can meet their protein requirements by consuming adequate amounts of these\n",
      "foods. Vegetarians and vegans can also attain their recommended protein intakes\n",
      "if they give a little more attention to high-quality plant-based protein\n",
      "sources. However, when following a vegetarian diet, the amino acid lysine can be\n",
      "challenging to acquire. Grains, nuts, and seeds are lysine-poor foods, but tofu,\n",
      "soy, quinoa, and pistachios are all good Proteins, Diet, and Personal Choices |\n",
      "417\n",
      "Page number: 417\n",
      "\n",
      "\n",
      "Score: 0.6634\n",
      "Text:\n",
      "Foods that contain some of the essential amino acids are called incomplete\n",
      "protein sources, while those that contain all nine essential amino acids are\n",
      "called complete protein sources, or high- quality protein sources. Foods that\n",
      "are complete protein sources include animal foods such as milk, cheese, eggs,\n",
      "fish, poultry, and meat, and a few plant foods, such as soy and quinoa. The only\n",
      "animal-based protein that is not complete is gelatin, which is made of the\n",
      "protein, collagen. Figure 6.18 Complete and Incomplete Protein Sources 414 |\n",
      "Proteins, Diet, and Personal Choices\n",
      "Page number: 414\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Better Visaulization of Results:\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "\n",
    "query = \"good foods for protein\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indices from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We could potentially improve the order of these results with a reranking model. A model that has been trained specifically to take search results (e.g. the top 25 semantic results) and rank them in order from most likely top-1 to least likely.\n",
    "\n",
    "See here for an open-source reranking model: https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1\n",
    "\n",
    "To check our results, what if we wanted to automatically surface the page of texts related to our query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 0,\n",
       "  'score': 0.99801946,\n",
       "  'text': \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\"},\n",
       " {'corpus_id': 2,\n",
       "  'score': 0.9969399,\n",
       "  'text': \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\"},\n",
       " {'corpus_id': 5,\n",
       "  'score': 0.0294786,\n",
       "  'text': \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model, here we use our base sized model: Re Ranking The Results:\n",
    "model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-large-v1\")\n",
    "\n",
    "# Example query and documents\n",
    "query = \"Who wrote 'To Kill a Mockingbird'?\"\n",
    "documents = [\n",
    "    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n",
    "    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n",
    "    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n",
    "    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n",
    "    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n",
    "    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n",
    "]\n",
    "\n",
    "# Lets get the scores\n",
    "results_ = model.rank(query, documents, return_documents=True, top_k=3)\n",
    "results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Page Number: 10\n",
      "Score: 0.99801946\n",
      "Text: 'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "Page Number: 30\n",
      "Score: 0.9969399\n",
      "Text: Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\n",
      "\n",
      "\n",
      "Document 3:\n",
      "Page Number: 60\n",
      "Score: 0.0294786\n",
      "Text: 'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example query and documents with page numbers\n",
    "query = \"Who wrote 'To Kill a Mockingbird'?\"\n",
    "documents = [\n",
    "    (\"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\", 10),\n",
    "    (\"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\", 20),\n",
    "    (\"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\", 30),\n",
    "    (\"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\", 40),\n",
    "    (\"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\", 50),\n",
    "    (\"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\", 60)\n",
    "]\n",
    "\n",
    "# Lets get the scores\n",
    "results = model.rank(query, [doc[0] for doc in documents], return_documents=True, top_k=3)\n",
    "\n",
    "# Print the results with page numbers\n",
    "for idx, doc in enumerate(results):\n",
    "    print(f\"Document {idx+1}:\")\n",
    "    print(\"Page Number:\", documents[doc['corpus_id']][1])\n",
    "    print(\"Score:\", doc['score'])\n",
    "    print(\"Text:\", doc['text'])\n",
    "    print(\"\\n\")  # Adding a new line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Re Ranking In Our Script:\n",
    "\n",
    "# # 1. Step Prepare the Documents:\n",
    "\n",
    "# query = \"good foods for protein\"\n",
    "# print(f\"Query: '{query}'\\n\")\n",
    "# print(\"Results:\")\n",
    "\n",
    "# related_documents = []\n",
    "# # Loop through top_results_dot_product and extract list on Index 1: Which is List of Indexes\n",
    "# for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):    \n",
    "#     related_documents.append(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "# print(f\"Related Documents: \\n {related_documents}\")    \n",
    "\n",
    "# # 2. Let's Get Score:\n",
    "# # Lets get the scores\n",
    "# results_reranked = model.rank(query, related_documents, return_documents=True, top_k=3)\n",
    "# for item in results_reranked:\n",
    "#     print(\"Score:\", item['score'])\n",
    "#     print(\"Text:\", item['text'])\n",
    "#     print(\"\\n\")  # Adding a new line for better readability\n",
    "    \n",
    "\n",
    "# Step 1: Prepare the Documents\n",
    "query = \"good foods for protein\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "\n",
    "related_documents_with_pages = []  # Combined list for documents and page numbers\n",
    "\n",
    "# Loop through top_results_dot_product and extract documents with page numbers\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    related_document = {\"text\": pages_and_chunks[idx][\"sentence_chunk\"],\n",
    "                        \"page_number\": pages_and_chunks[idx][\"page_number\"]}\n",
    "    related_documents_with_pages.append(related_document)\n",
    "\n",
    "# Step 2: Get Scores and Re-Rank\n",
    "results_reranked = model.rank(query, [doc['text'] for doc in related_documents_with_pages], return_documents=True, top_k=3)\n",
    "\n",
    "# Print re-ranked results with page numbers\n",
    "for idx, item in enumerate(results_reranked):\n",
    "    print(f\"Rank {idx+1}:\")\n",
    "    print(\"Page Number:\", related_documents_with_pages[item['corpus_id']]['page_number'])\n",
    "    print(\"Score:\", item['score'])\n",
    "    print(\"Text:\", item['text'])\n",
    "    print(\"\\n\")  # Adding a new line for better readability\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open PDF and load target\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc.load_page(411 + 41) # note: page numbers of our PDF start 41+\n",
    "\n",
    "# Get the image of the page\n",
    "img = page.get_pixmap(dpi=300)\n",
    "\n",
    "# Save image (optional)\n",
    "# img.save(\"output_filename.png\")\n",
    "doc.close()\n",
    "\n",
    "# Convert the pixmap to a numpy array\n",
    "img_array = np.frombuffer(img.samples_mv,\n",
    "                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
    "\n",
    "# Display the image using Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(13, 10))\n",
    "plt.imshow(img_array)\n",
    "plt.title(f\"Query: '{query}' | Most relevant page:\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Similarity Measures --------------------------------\n",
    "\n",
    "Cosine Similarity: A⋅B/∥A∥⋅∥B∥ -- Where A⋅B is the dot product of vectors & ∥A∥ and ∥𝐵∥ are the Euclidean norms (magnitudes) of vectors A and 𝐵, respectively.\n",
    "\n",
    "Example: Let's consider two vectors: (A = [3, 4]) and (B = [1, 2]).\n",
    "\n",
    "- A⋅B=(3×1)+(4×2)=3+8=11\n",
    "- ∥A∥ = SQRT((3)**2 + (4)**2)  = 5\n",
    "- ∥B∥= SQRT((1)**2 + (2)**2) = SQRT(5)\n",
    "- Cosine Similarity: A⋅B/∥A∥⋅∥B∥ = 11/(5xSQRT(5)) = 0.984\n",
    "- Interpreation: A cosine similarity value close to 1 indicates that the vectors have a high degree of similarity in direction, meaning they are pointing in similar directions within the vector space. In our example, vectors 𝐴 and  𝐵 are relatively similar in direction, leading to a high cosine similarity value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Similarity Measures:\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = torch.dot(vector1, vector2)\n",
    "\n",
    "    # Get Euclidean/L2 norm\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "# Example vectors/tensors\n",
    "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
    "\n",
    "# Calculate dot product\n",
    "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
    "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
    "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
    "\n",
    "# Cosine similarity\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Cosine similarity Implemented in Numpy\")\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity\n",
    "\n",
    "# Example vectors\n",
    "vector_a = np.array([3, 5, 2])\n",
    "vector_b = np.array([1, 4, 7])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity(vector_a, vector_b)\n",
    "print(f\"Cosine Similarity: {cos_sim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fledged Functionizing our Semantic Search Pipeline:\n",
    "\n",
    "-- Our Pipeline Contains Two Model First is for Embedding Creation & Second is for Raranking the Result. You can Also Remove the Reranking Steps:\n",
    "\n",
    "-- Also We Provide by default because we already downloaded the models and imported in this scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, here we use our base sized model\n",
    "rerank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-large-v1\", device=\"cuda\")\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=\"cuda\")    # Download from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model_encode,\n",
    "                                n_resources_to_return,\n",
    "                                print_time: bool = True):\n",
    "    \"\"\"\n",
    "    Embeds a query with a model and returns top-k scores and indices from embeddings.\n",
    "    \n",
    "    Args:\n",
    "    - query: The input query string.\n",
    "    - embeddings: Torch tensor containing embeddings of all resources.\n",
    "    - model_encode: SentenceTransformer model for encoding queries .\n",
    "    - n_resources_to_return: Number of top resources to return after searching .\n",
    "    - print_time: Boolean flag to print time taken for computation (default: True).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: Torch tensor containing top-k scores.\n",
    "    - indices: Torch tensor containing indices corresponding to top-k scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = model_encode.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on ({len(embeddings)} embeddings): {end_time - start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)\n",
    "    \n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model_encode: SentenceTransformer = embedding_model,\n",
    "                                model_rerank: CrossEncoder = rerank_model,\n",
    "                                n_resources_to_return: int = 5,\n",
    "                                n_resources_to_return_rerank: int = 5,\n",
    "                                pages_and_chunks: list[dict] = pages_and_chunks):\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds relevant passages given a query and prints them out along with their scores.\n",
    "    \"\"\"\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  model_encode=model_encode,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    related_documents_with_pages = []\n",
    "    \n",
    "    # Loop through top_results_dot_product and extract documents with page numbers\n",
    "    for score, idx in zip(scores, indices):\n",
    "        related_document = {\"text\": pages_and_chunks[idx][\"sentence_chunk\"],\n",
    "                            \"page_number\": pages_and_chunks[idx][\"page_number\"]}\n",
    "        related_documents_with_pages.append(related_document)\n",
    "        \n",
    "    # Step 2: Get Scores and Re-Rank\n",
    "    results_reranked = model_rerank.rank(query, [doc['text'] for doc in related_documents_with_pages], return_documents=True, top_k=n_resources_to_return_rerank)\n",
    "    \"\"\"Results ReRanked Returns: List of Dict with Keys Name like: corpus id, text\"\"\"\n",
    "\n",
    "    # Print re-ranked results with page numbers\n",
    "    for idx, item in enumerate(results_reranked):\n",
    "        print(f\"Rank {idx+1}:\")\n",
    "        print(\"Page Number:\", related_documents_with_pages[item['corpus_id']]['page_number'])\n",
    "        print(\"Score:\", item['score'])\n",
    "        print(\"Text:\", item['text'])\n",
    "        print(\"\\n\")  # Adding a new line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of Above Pipeline:\n",
    "query=\"foods high in fiber\"\n",
    "print_top_results_and_scores(query=query, \n",
    "                             embeddings=embeddings,\n",
    "                             model_encode=embedding_model,\n",
    "                             model_rerank=rerank_model,\n",
    "                             n_resources_to_return=5,\n",
    "                             n_resources_to_return_rerank=3,\n",
    "                             pages_and_chunks=pages_and_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generate from LLM based on our Enocded Query and Context & Paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking our local GPU memory availability so Based on that we can load our Model:\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n",
    "gpu_memory_bytes = None  # Free the Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an LLM locally\n",
    "\n",
    "We can load an LLM locally using Hugging Face `transformers`.\n",
    "\n",
    "To get a model running local we're going to need a few things:\n",
    "1. A quantization config (optional) - a config on what precision to load the model in (e.g. 8bit, 4bit, etc)\n",
    "2. A model ID - this will tell transformers which model/tokenizer to load\n",
    "3. A tokenizer - this turns text into numbers ready for the LLM (note: a tokenizer is different from an embedding model)\n",
    "4. An LLM model - this will be what we use to generate text based on an input!\n",
    "\n",
    "> **Note:** There are many tips and tricks on loading/making LLMs work faster. One of the best ones is flash_attn (Flash Attention 2). See the GitHub for more: https://github.com/Dao-AILab/flash-attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "# from flash_attn import flash_attn_qkvpacked_func, flash_attn_func  # Not Supported in Our Hardware so\n",
    "\n",
    "# 1. Create a quantization config\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "#                                          bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # Load the model weights in 4-bit precision (instead of the default 8-bit precision) to save memory) -  (Weights & Activatin.. So Significant Reduce Model Memory Footprint Less 16x Mmemory and 2X Fatser Inference )\n",
    "    bnb_4bit_use_double_quant=True, # Use double quantization for 4-bit weights # Nested Quantization # which can improve accuracy.\n",
    "    bnb_4bit_quant_type=\"nf4\",      # Use the \"nf4\" quantization type (NVIDIA's 4-bit floating-point format) # Normalized Float 4\n",
    "    bnb_4bit_compute_dtype=torch.float16 # Perform computations using float16 (half-precision)\n",
    ")\n",
    "\n",
    "# Bonus: flash attention 2 = faster attention mechanism\n",
    "# Flash Attention 2 requires a GPU with a compute capability score of 8.0+ (Ampere, Ada Lovelace, Hopper and above): https://developer.nvidia.com/cuda-gpus \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\" # scaled dot product attention\n",
    "print(f\"Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "\n",
    "# 2. Pick a model we'd like to use\n",
    "# This is to download pre trained models. but we have already downloaded that. but which is not gemma\n",
    "model_id = \"microsoft/phi-1_5\"\n",
    "model_id = model_id\n",
    "model_id = r\"C:\\Coding\\LLM\\phi-1_5\"\n",
    "tokenizer_id = r\"microsoft/phi-1_5\"\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=tokenizer_id, trust_remote_code=True)\n",
    "\n",
    "# Set the `pad_token` attribute of the tokenizer to be the same as the `eos_token` (end-of-sequence token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4. Instantiate the model \n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map={\"\":0},\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 low_cpu_mem_usage=True, # use as much memory as we can\n",
    "                                                 attn_implementation=attn_implementation)\n",
    "\n",
    "# if not bnb_config:\n",
    "#     llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture:\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Getting Model Params:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_num_params\u001b[39m(model: \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      3\u001b[0m     params \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28msum\u001b[39m([param\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()])\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo. of Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Getting Model Params:\n",
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    params =  sum([param.numel() for param in model.parameters()])\n",
    "    print(f\"No. of Parameters: {params}\")\n",
    "    params = None # Svae the Mmemory\n",
    "    \n",
    "\n",
    "get_model_num_params(llm_model)\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "# Getting Model Size:\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    # Calculate memory size of model parameters\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    \n",
    "    # Calculate memory size of model buffers\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate total model memory size in bytes, megabytes, and gigabytes\n",
    "    model_mem_bytes = mem_params + mem_buffers\n",
    "    model_mem_mb = model_mem_bytes / (1024**2)\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) \n",
    "\n",
    "    # Print the memory sizes in a readable format\n",
    "    print({\n",
    "        \"model_mem_bytes\": f\"{model_mem_bytes} Bytes\",\n",
    "        \"model_mem_mb\": f\"{round(model_mem_mb, 2)} MB\", \n",
    "        \"model_mem_gb\": f\"{round(model_mem_gb, 2)} GB\"\n",
    "    })\n",
    "    \n",
    "    # Clear memory variables to release memory resources\n",
    "    mem_params, mem_buffers = None, None\n",
    "    model_mem_bytes, model_mem_mb, model_mem_gb = None, None, None   \n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating The Text from Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So Our Model is Not Suppor Chat Format So Directly we apply the:\n",
    "\n",
    "# input_text = \"What are the macronutrients and what are their functions in the body?\"\n",
    "# print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# # Create prompt template for instruction-tuned model\n",
    "# dialogue_template = [\n",
    "#     {\"role\": \"user\",\n",
    "#      \"content\": input_text}\n",
    "# ]\n",
    "\n",
    "# # Apply the chat template\n",
    "# prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "#                                        tokenize=False,\n",
    "#                                        add_generation_prompt=True)\n",
    "# print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inputs = tokenizer('What are the macronutrients and what are their functions in the body?', \n",
    "                   return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "outputs = llm_model.generate(**inputs, max_length=250)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(f\"Decoded Token by Model: \\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Decoding Method:\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert our embeddings into a torch.tensor\n",
    "embeddings = torch.tensor(np.stack(text_chunks_and_embedding_df[\"embedding\"].tolist(), axis=0), dtype=torch.float32).to(device)\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "text_chunks_and_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, here we use our base sized model\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "rerank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-large-v1\", device=\"cuda\")\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=\"cuda\")    # Download from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
    "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
    "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"Explain the concept of energy balance and its importance in weight management.\"]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"How often should infants be breastfed?\",\n",
    "    \"What are symptoms of pellagra?\",\n",
    "    \"How does saliva help with digestion?\",\n",
    "    \"What is the RDI for protein per day?\",\n",
    "    \"water soluble vitamins\"]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model_encode=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool = True):\n",
    "    \"\"\"\n",
    "    Embeds a query with a model and returns top-k scores and indices from embeddings.\n",
    "    \n",
    "    Args:\n",
    "    - query: The input query string.\n",
    "    - embeddings: Torch tensor containing embeddings of all resources.\n",
    "    - model_encode: SentenceTransformer model for encoding queries .\n",
    "    - n_resources_to_return: Number of top resources to return after searching .\n",
    "    - print_time: Boolean flag to print time taken for computation (default: True).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: Torch tensor containing top-k scores.\n",
    "    - indices: Torch tensor containing indices corresponding to top-k scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = model_encode.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on ({len(embeddings)} embeddings): {end_time - start_time:.5f} seconds.\")\n",
    "    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)\n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def prompt_formatter(query: str,\n",
    "                     context_items: list[dict]) -> str:\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    Make sure your answers are as explanatory as possible.\n",
    "    Use the following examples as reference for the ideal answer style.\n",
    "    \\nExample 1:\n",
    "    Query: What are the fat-soluble vitamins?\n",
    "    Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "    \\nExample 2:\n",
    "    Query: What are the causes of type 2 diabetes?\n",
    "    Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "    \\nExample 3:\n",
    "    Query: What is the importance of hydration for physical performance?\n",
    "    Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "    \\nNow use the following context items to answer the user query:\n",
    "    {context}\n",
    "    \\nRelevant passages: <extract relevant passages from the context here>\n",
    "    User query: {query}\n",
    "    Answer:\"\"\" \n",
    "    base_prompt = base_prompt.format(context=context,\n",
    "                                     query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model \n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": base_prompt}]\n",
    "    \n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = random.choice(query_list) \n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "\n",
    "# Create a list of context items\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format our prompt\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # from 0 to 1 and the lower the value, the more deterministic the text, the higher the value, the more creative\n",
    "                             do_sample=True, # whether or not to use sampling, https://huyenchip.com/2024/01/16/sampling.html\n",
    "                             max_new_tokens=256)\n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\m{output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=qN_2fnOPY-M&t=14944s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
